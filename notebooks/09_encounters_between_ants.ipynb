{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encounters between ants\n",
    "Function to calculate the encounters between specific ants (e.g., focal and caregiver ants) from a ```.mymridon``` experiment file. <br><br>\n",
    "There is probably an easier way to do this by querying individual frames directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta  # For convenient handling of time and date\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd  # Used to create a dataframe, similar to the structure used in R\n",
    "import py_fort_myrmidon as fm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to output trajectories of all ants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trajectory_output_all(start_time, end_time, exp):\n",
    "    \"\"\"\n",
    "    Function to extract daily trajectories, grouped by AntID. While it is setup to extract daily trajectories, it can work for any arbitrary time duration\n",
    "    :param start_time: The start datetime object. this will be converted to a fort-myrmidon Time object\n",
    "    :param end_time: The end datetime object. this will be converted to a fort-myrmidon Time object\n",
    "    :param exp: The name of the experiment i.e., the myrmidon file\n",
    "    :param matcher_query: The fm matcher corresponding to the focal IDs\n",
    "    :return: Outputs a pandas dataframe containing AntID, Space, Time, X_coordinates and Y_coordinates of each ID averaged over 1 second from the X and Y coordinates. Averagingg is done to have a dataset which can be merged across IDs using at the resolution of 1s.\n",
    "    \"\"\"\n",
    "    start = datetime.now()\n",
    "    t_begin = fm.Time(start_time)\n",
    "    t_stop = fm.Time(end_time)\n",
    "    trajectory = fm.Query.ComputeAntTrajectories(\n",
    "        experiment=exp,\n",
    "        start=t_begin,\n",
    "        end=t_stop,\n",
    "        # matcher=matcher_query,\n",
    "        maximumGap=fm.Duration.Parse(\"1000h\"),\n",
    "        reportProgress=False,\n",
    "    )\n",
    "    # Make a list of lists with trajectory values needed. Position is an array of 5 columns, so specific columns are called\n",
    "    traj_list = [\n",
    "        [\n",
    "            trajectory.Ant,\n",
    "            trajectory.Space,\n",
    "            trajectory.Start.ToDateTime(),\n",
    "            trajectory.Positions[:, 0],\n",
    "            trajectory.Positions[:, 1],\n",
    "            trajectory.Positions[:, 2],\n",
    "        ]\n",
    "        for trajectory in trajectory\n",
    "    ]\n",
    "    # Make the list into a dataframe\n",
    "    traj_df = pd.DataFrame(\n",
    "        traj_list,\n",
    "        columns=[\"AntID\", \"Space\", \"StartTime\", \"Pos_time\", \"X_coor\", \"Y_coor\"],\n",
    "    )\n",
    "    # Explode columns which are in the form of lists to expand the dataframe\n",
    "    traj_df = traj_df.explode(column=[\"Pos_time\", \"X_coor\", \"Y_coor\"])\n",
    "    # Coerce coordinates to integer\n",
    "    traj_df[\"X_coor\"] = pd.to_numeric(traj_df[\"X_coor\"], errors=\"coerce\")\n",
    "    traj_df[\"Y_coor\"] = pd.to_numeric(traj_df[\"Y_coor\"], errors=\"coerce\")\n",
    "    # Convert Pos_time to timedelta and obtain actual datetime for each trajectory entry\n",
    "    traj_df[\"Pos_time\"] = pd.to_numeric(traj_df[\"Pos_time\"], errors=\"coerce\")\n",
    "    traj_df[\"Pos_time\"] = pd.to_timedelta(\n",
    "        traj_df[\"Pos_time\"], unit=\"S\", errors=\"coerce\"\n",
    "    )\n",
    "    traj_df[\"Time\"] = traj_df[\"StartTime\"] + traj_df[\"Pos_time\"]\n",
    "    # Drop unwanted ccolumns\n",
    "    traj_df = traj_df.drop([\"StartTime\", \"Pos_time\"], axis=1)\n",
    "    # Reorder columns\n",
    "    traj_df = traj_df[[\"AntID\", \"Space\", \"Time\", \"X_coor\", \"Y_coor\"]]\n",
    "    if traj_df.empty:  # If no trajectories are output\n",
    "        # empty_row = pd.DataFrame([{'AntID': 'Unknown', 'Space':np.nan, 'Time':np.nan, 'X_coor':np.nan, 'Y_coor':np.nan}]) # Create empty row with unknown as antID\n",
    "        # traj_df = pd.concat([empty_row]) # Add empty row to dataframe\n",
    "        print(\"No trajectories found. Created empty dataframe\")\n",
    "        return traj_df  # Return empty dataframe\n",
    "    # Obtain average X and Y coordinates per second\n",
    "    # traj_df = (\n",
    "    #     traj_df.groupby([pd.Grouper(key=\"Time\", freq=\"1s\"), \"AntID\", \"Space\"])\n",
    "    #     .agg(X_mean=(\"X_coor\", \"mean\"), Y_mean=(\"Y_coor\", \"mean\"))\n",
    "    #     .reset_index()\n",
    "    # )\n",
    "    end = datetime.now()\n",
    "    # print(\"Trajectories output in\", end-start)\n",
    "    return traj_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate duration of encounters\n",
    "This function will calculate the duration of time an ant spends in the encounter zone after moving from the away zone to the encounter zone. This will require first identifying instances of an encounter, then calculating duration when the ant is within the encounter threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run over each sequence of encounters to extract sub-sequences where individuals are within the encounter threshold (enc_dummy=1).\n",
    "# The number and total duration of these sequences is output\n",
    "def time_within_encounter_threshold(displacement_dataset, encounter_sequence):\n",
    "    \"\"\"Function to quantify sub-sequences when an individual is within the encounter threshold during an encounter.\n",
    "    These sub-sequences have a enc_dummy value of 1. The function extracts these sub-sequences, obtains their start and end time points\n",
    "    from the displacment dataset and outputs the number and total duration of all sub-sequences within an encounter sequence.\n",
    "    Note that the encounter sequence should only contain values of 0.5 and 1, since an encounter by definition is within two instances of enc_dummy == 0\n",
    "\n",
    "    Args:\n",
    "        displacement_dataset (pandas.DataFrame): A pandas dataframe which contains at least the main index values and a column with Time\n",
    "        encounter_sequence (numpy.array): A numpy array containing the sequence of enc_dummy values with 0.5 and 1 corresponding to when displacement is within away threshold and within encounter threshold\n",
    "\n",
    "    Returns:\n",
    "        total_enc (int): The number of sub-sequences which are within the encounter threshold\n",
    "        total_enc (numpy.float64): The total duration of all sub-sequences which are within the encounter threshold\n",
    "    \"\"\"\n",
    "    # Add 0.5 to end of the list, then find out indices where consecutive difference is not 0\n",
    "    change_indices = np.where(\n",
    "        np.diff(np.concatenate(([0.5], encounter_sequence, [0.5]))) != 0\n",
    "    )[0]\n",
    "    # Get start indices starting from the bginning and jumping by 2 upto the end\n",
    "    start_indices = [encounter_sequence.index.values[x] for x in change_indices[::2]]\n",
    "    # Get end indices starting from 1 going upto the end jumping by 2. Subtract 1 from all index values to account for the extra value added at the concatenation step\n",
    "    end_indices = [encounter_sequence.index.values[x] for x in change_indices[1::2] - 1]\n",
    "    # Get time of starting indices\n",
    "    start_times = [displacement_dataset.loc[x, \"Time\"] for x in start_indices]\n",
    "    # Get time of ending indices\n",
    "    end_times = [displacement_dataset.loc[x, \"Time\"] for x in end_indices]\n",
    "    # Get time duration within encounter thresholds by subtraction\n",
    "    enc_times = np.subtract(end_times, start_times)\n",
    "    enc_times_sec = [x.total_seconds() for x in enc_times]\n",
    "    # Obtain total time and number of times ants are within encounter threshold consecutively\n",
    "    total_enc_times = np.sum(enc_times_sec)\n",
    "    total_enc = len(enc_times_sec)\n",
    "    return total_enc, total_enc_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encounter_duration(displacement_dataset, encounter_threshold, away_threshold):\n",
    "    \"\"\"Function to calculate duration of encounters between a focal ant and another individual.\n",
    "    Encounters are defined as when an ant moves from beyond the away threshold to within the encounter threshold with respect to the focal ant.\n",
    "    Two metrics of encounter duration are calculated - one based on the total time from when an ant moves within the encounter threshold to when it crosses the away threshold again\n",
    "    and another based on only the total time spent within the encounter threshold during the encounter\n",
    "    Outputs a dataframe with these durations calculated for each encounter between the 2 ants\n",
    "\n",
    "    Args:\n",
    "        displacement_dataset (pandas.DataFrame): A pandas dataframe containing at least Timestamps and displacement between 2 ants at each timestamp. It should also contain an index which is used to match sequence starts and stops to timestamps\n",
    "        encounter_threshold (int): The value of displacement for the encounter threshold\n",
    "        away_threshold (int): The value of displacement for the away threshold\n",
    "\n",
    "    Returns:\n",
    "        enc_df(pandas.DataFrame): A dataframe containing the number of the encounter, the start time, total duration,\n",
    "    number of times within the encounter where the displacement between the two ants were within the encounter threshold and the total duration of these phases\n",
    "    \"\"\"\n",
    "    # First interpolate (linearly) missing displacement values\n",
    "    displacement_dataset[\"disp\"].interpolate(\n",
    "        method=\"linear\", limit_direction=\"forward\", inplace=True\n",
    "    )\n",
    "    # Create a new column based on converting the thresholds to dummy numbers. Values given are 1, if displacement < encounter threshold, 0.5, if encounter threshold < disp < away threshold and 0 if disp > away threshold.\n",
    "    # Due to the linear interpolation there are no np.nans\n",
    "    displacement_dataset.loc[:, [\"enc_dummy\"]] = pd.cut(\n",
    "        displacement_dataset.disp,\n",
    "        [0, encounter_threshold, away_threshold, np.inf],\n",
    "        labels=[1, 0.5, 0],\n",
    "    )\n",
    "    # Convert datatype to float from category (due to pd.cut) for downstream functions\n",
    "    displacement_dataset = displacement_dataset.astype({\"enc_dummy\": float})\n",
    "    # Check if 1 is present at least once in the dataset. If not create a dataframe with 0 values to return\n",
    "    if 1 not in displacement_dataset.enc_dummy.values:\n",
    "        enc_df = pd.DataFrame(data=[[0, np.nan, 0.0, 0, 0.0]])\n",
    "        enc_df.columns = [\n",
    "            \"enc_number\",\n",
    "            \"enc_start_time\",\n",
    "            \"enc_duration\",\n",
    "            \"enc_sequences\",\n",
    "            \"enc_sequences_duration\",\n",
    "        ]\n",
    "        return enc_df\n",
    "    # Identify index values where enc_dummy is 0 i.e., displacement is > away threshold\n",
    "    away_indices = displacement_dataset[\n",
    "        displacement_dataset[\"enc_dummy\"] == 0\n",
    "    ].index.values\n",
    "    # Check if there is none or only 1 index with enc_dummy=0\n",
    "    if away_indices.size < 2:\n",
    "        enc_df = pd.DataFrame(data=[[0, np.nan, 0.0, 0, 0.0]])\n",
    "        enc_df.columns = [\n",
    "            \"enc_number\",\n",
    "            \"enc_start_time\",\n",
    "            \"enc_duration\",\n",
    "            \"enc_sequences\",\n",
    "            \"enc_sequences_duration\",\n",
    "        ]\n",
    "        return enc_df\n",
    "    # Check if first value is not starting index and insert it if not\n",
    "    if away_indices[0] != np.take(displacement_dataset.index.values, 0):\n",
    "        away_indices = np.insert(\n",
    "            away_indices, 0, np.take(displacement_dataset.index.values, 0)\n",
    "        )  # Insert starting value of index range\n",
    "    # Get pairs of consecutive indices\n",
    "    away_indices_pair = list(zip(away_indices, away_indices[1:]))\n",
    "    # Get values between the consecutive indices with value of 1\n",
    "    seq_bw_away = [\n",
    "        displacement_dataset.loc[x + 1 : y - 1, \"enc_dummy\"]\n",
    "        for (x, y) in away_indices_pair\n",
    "    ]\n",
    "    # Remove list elements which have only 1 index value\n",
    "    seq_bw_away_sub = [x for x in seq_bw_away if x.size > 1]\n",
    "    # Subset list elements which have at least 1 value of 1\n",
    "    # This list of lists will be used for all subsequent calculations\n",
    "    enc_seq = [x for x in seq_bw_away_sub if np.in1d(1, x)]\n",
    "\n",
    "    # Calculate overall encounter duration as time from when an individual crosses threshold (enc_dummy=1) to when it crosses away threshold again (enc_dummy=0)\n",
    "    # Get index values where 1 is present for first time in each list within encounter sequences.\n",
    "    # Then use this to obtain the start times of the encounters\n",
    "    enc_start_times = [\n",
    "        displacement_dataset.loc[x.index.values[np.where(x == 1)[0][0]], \"Time\"]\n",
    "        for x in enc_seq\n",
    "    ]\n",
    "    # Get index values of last time point in each encounter.\n",
    "    # Then use this to obtain the end times of the whole encounters\n",
    "    enc_end_times = [\n",
    "        displacement_dataset.loc[x.index.values[-1], \"Time\"] for x in enc_seq\n",
    "    ]\n",
    "    # Subtract end and start times element wise and convert to seconds\n",
    "    enc_time = np.subtract(enc_end_times, enc_start_times)\n",
    "    enc_time_sec = [x.total_seconds() for x in enc_time]\n",
    "\n",
    "    # Calculate number of instances within each encounter where the individual is within encounter threshold for a sequential period of time and the total duration of these instances\n",
    "    # This is based on extracting all consecutive sequences where enc_dummy=1 and calculating the start and end time of these sequences\n",
    "    # The function `time_within_encounter_threshold` runs over each encounter sequence to extract the sub-sequences\n",
    "    enc_sub_seq = [\n",
    "        time_within_encounter_threshold(displacement_dataset, x) for x in enc_seq\n",
    "    ]\n",
    "    enc_sub_seq_num = [x for x, y in enc_sub_seq]\n",
    "    enc_sub_seq_time = [y for x, y in enc_sub_seq]\n",
    "\n",
    "    # Create a dataframe combining all the paeameters\n",
    "    # First calculate number of encounters\n",
    "    enc_num = np.arange(1, len(enc_seq) + 1)\n",
    "    # Create a dataframe\n",
    "    enc_df = pd.DataFrame(\n",
    "        data=[enc_num, enc_start_times, enc_time_sec, enc_sub_seq_num, enc_sub_seq_time]\n",
    "    )\n",
    "    # transpose dataframe\n",
    "    enc_df = enc_df.T\n",
    "    # Add column names\n",
    "    enc_df.columns = [\n",
    "        \"enc_number\",\n",
    "        \"enc_start_time\",\n",
    "        \"enc_duration\",\n",
    "        \"enc_sequences\",\n",
    "        \"enc_sequences_duration\",\n",
    "    ]\n",
    "    enc_df = enc_df.astype(\n",
    "        {\n",
    "            \"enc_number\": int,\n",
    "            \"enc_duration\": float,\n",
    "            \"enc_sequences\": int,\n",
    "            \"enc_sequences_duration\": float,\n",
    "        }\n",
    "    )\n",
    "    return enc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate encounter durations between specific ants\n",
    "\n",
    "This function combines obtaining all the trajectories from the `trajectory_output_all` function, then rearranges this to obtain the displacement between the focal ant and all other ants at each time point (in this every second, since the trajectories are summarised to the nearest second by averaging the X and Y coordinate for each second). This dataset is then grouped by focalID and antID and then the `encounter_duration` function is run over these groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def focal_encounters(\n",
    "    start_time, end_time, exp, focal_ID, exp_day, encounter_threshold, away_threshold\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to obtain trajectories for focal and caregiver antIDs, merge by time and calculate displacement of each caregiver ID from the focal ID at every second\n",
    "    :param start_time: Starting time to obtain trajectories from. Passed on to function trajectory_output\n",
    "    :param end_time: Ending time to obtain trajectories from. Passed on to function trajectory_output\n",
    "    :param exp: Location of myrmidon file\n",
    "    :param focal_ID: Injured AntID\n",
    "    :param exp_day: Day of the experiment. This is added to the dataframe for identification\n",
    "    :param encounter_threshold: Threshold displacement to use as encounter\n",
    "    :param away_threshold: Threshold displacement to use as the start/end of an encounter\n",
    "    :return: Returns a datafarme containing the Time (in bins of 1s based on function trajectory_output), the focal and caregiver ID, the space in which the focal and caregiver ants are present, and the displacement between them (calculated as np.nan if they are in different spaces. In a CSV output this will be converted to a blank entry).\n",
    "    \"\"\"\n",
    "    start = datetime.now()\n",
    "    # # Focal Ant matcher\n",
    "    # focal_matcher = fm.Matcher.AntID(focal_ID)\n",
    "    # # Caregiver individual matchers\n",
    "    # # others = [fm.Matcher.AntID(x) for x in other_IDs]\n",
    "    # # Create single matcher object by unpacking the list within an Or Matcher\n",
    "    # #others_matcher = fm.Matcher.Or(*others)\n",
    "    # # Focal Ant trajectory\n",
    "    # focal_traj = trajectory_output(start_time, end_time, exp, focal_matcher)\n",
    "    # All ant trajectories\n",
    "    other_traj = trajectory_output_all(start_time, end_time, exp)\n",
    "    # Focal ant trajectory\n",
    "    focal_traj = other_traj[other_traj[\"AntID\"] == focal_ID]\n",
    "    # Sort Time column for both dataframes\n",
    "    other_traj = other_traj.sort_values(\"Time\")\n",
    "    focal_traj = focal_traj.sort_values(\"Time\")\n",
    "\n",
    "    # If focal trajectory is an empty dataframe, create a dataframe with na values for encounter parameters\n",
    "    if focal_traj.empty:\n",
    "        full_traj = other_traj.rename(columns={\"Space\": \"Space_ant\"})\n",
    "        full_traj[\"focalID\"] = focal_ID\n",
    "        full_traj[\"Space_focal\"] = full_traj[\"disp\"] = (\n",
    "            np.nan\n",
    "        )  # Create columns with na values\n",
    "        full_traj = full_traj[\n",
    "            [\"Time\", \"focalID\", \"AntID\", \"disp\", \"Space_focal\", \"Space_ant\"]\n",
    "        ]\n",
    "        full_traj[\"exp_day\"] = exp_day\n",
    "        full_traj = full_traj[full_traj[\"focalID\"] != full_traj[\"AntID\"]].reset_index()\n",
    "        # Group data frame, create columns with na and output encounter dataframe\n",
    "        enc_df = (\n",
    "            full_traj.groupby([\"exp_day\", \"focalID\", \"AntID\"])\n",
    "            .apply(lambda x: pd.Series([np.nan] * 5))\n",
    "            .reset_index()\n",
    "            .rename(\n",
    "                columns={\n",
    "                    0: \"enc_number\",\n",
    "                    1: \"enc_start_time\",\n",
    "                    2: \"enc_duration\",\n",
    "                    3: \"enc_sequences\",\n",
    "                    4: \"enc_sequences_duration\",\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            f\"{'Focal ID trajectory is empty for list item '}{exp_day}{' .Returning dataframe with no displacement and encounters calculated'}\"\n",
    "        )\n",
    "        return enc_df\n",
    "    # If trajectory of all other individuals is an empty dataframe, create a dataframe with na values for encounter parameters\n",
    "    if other_traj.empty:\n",
    "        full_traj = focal_traj.rename(\n",
    "            columns={\"AntID\": \"focalID\", \"Space\": \"Space_focal\"}\n",
    "        )\n",
    "        full_traj[\"AntID\"] = full_traj[\"Space_ant\"] = full_traj[\"disp\"] = (\n",
    "            np.nan\n",
    "        )  # Create columns with na values\n",
    "        full_traj = full_traj[\n",
    "            [\"Time\", \"focalID\", \"AntID\", \"disp\", \"Space_focal\", \"Space_ant\"]\n",
    "        ]\n",
    "        full_traj[\"exp_day\"] = exp_day\n",
    "        # Group data frame, create columns with na and output encounter dataframe\n",
    "        enc_df = (\n",
    "            full_traj.groupby([\"exp_day\", \"focalID\", \"AntID\"])\n",
    "            .apply(lambda x: pd.Series([np.nan] * 5))\n",
    "            .reset_index()\n",
    "            .rename(\n",
    "                columns={\n",
    "                    0: \"enc_number\",\n",
    "                    1: \"enc_start_time\",\n",
    "                    2: \"enc_duration\",\n",
    "                    3: \"enc_sequences\",\n",
    "                    4: \"enc_sequences_duration\",\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            f\"{'Caregiver ID trajectories are empty for list item '}{exp_day}{' .Returning dataframe with no displacement and encounters calculated'}\"\n",
    "        )\n",
    "        return enc_df\n",
    "\n",
    "    # Merge focal and caregiver trajectories on Time column using merge_asof to match nearest time values\n",
    "    full_traj = pd.merge_asof(\n",
    "        other_traj,\n",
    "        focal_traj,\n",
    "        on=\"Time\",\n",
    "        suffixes=(\"_ant\", \"_focal\"),\n",
    "        direction=\"nearest\",\n",
    "        tolerance=pd.Timedelta(\"1s\"),\n",
    "    )\n",
    "    # Obtain X coordinate and Y coordinate difference between Focal and Caregivers, for each row\n",
    "    full_traj[\"X_diff\"] = full_traj[\"X_coor_focal\"] - full_traj[\"X_coor_ant\"]\n",
    "    full_traj[\"Y_diff\"] = full_traj[\"Y_coor_focal\"] - full_traj[\"Y_coor_ant\"]\n",
    "    # Obtain displacement\n",
    "    full_traj[\"disp\"] = np.linalg.norm(\n",
    "        full_traj[[\"X_diff\", \"Y_diff\"]].to_numpy(), axis=1\n",
    "    )\n",
    "    # Rename columns\n",
    "    full_traj = full_traj.rename(\n",
    "        columns={\"AntID_focal\": \"focalID\", \"AntID_ant\": \"AntID\"}\n",
    "    )\n",
    "    # Subset specific columns\n",
    "    full_traj = full_traj[\n",
    "        [\"Time\", \"focalID\", \"AntID\", \"disp\", \"Space_focal\", \"Space_ant\"]\n",
    "    ]\n",
    "    # Add experimental day\n",
    "    full_traj[\"exp_day\"] = exp_day\n",
    "    # Remove instances where the focal ant's displacement is calculated wrt itself.\n",
    "    full_traj = full_traj[full_traj[\"focalID\"] != full_traj[\"AntID\"]].reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    # Replace with arbitrary high value of displacemeent if focal ant and caregiver are in different spaces. Use notnull to filter out instances where focal or caregiver space is not known. The higgh value will ensure that this case is always considered as > away_threshold in count_encounters function\n",
    "    full_traj.loc[\n",
    "        (\n",
    "            (full_traj.Space_focal.notnull())\n",
    "            & (full_traj.Space_ant.notnull())\n",
    "            & (full_traj.Space_focal != full_traj.Space_ant)\n",
    "        ),\n",
    "        \"disp\",\n",
    "    ] = 100000\n",
    "    # Apply encounter_duration function over grouped dataframe, reset index and rename columns\n",
    "    enc_df = (\n",
    "        full_traj.groupby([\"exp_day\", \"focalID\", \"AntID\"])\n",
    "        .apply(lambda x: encounter_duration(x, encounter_threshold, away_threshold))\n",
    "        .reset_index()\n",
    "        .drop(\"level_3\", axis=1)\n",
    "    )\n",
    "    end = datetime.now()\n",
    "    print(\n",
    "        f\"{'Encounters for experimental day '}{exp_day}{' calculated in '}{end - start}\"\n",
    "    )\n",
    "    return enc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to run encounter calculations over multiple phases of one experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_encounters_cluster(exp_list, control_list, pre_list, post_list, colonyID):\n",
    "    \"\"\"\n",
    "    Helper function to run focal_caregivers_disp_exp and count_encounters over multiple lists associated with different phases for one colony\n",
    "    :param exp_list: List of start_time, end_time, exp, focalID, careggiverIDs and exp_day for Experimental phase\n",
    "    :param control_list: List for control phase\n",
    "    :param pre_list: List for pre-experimental phase\n",
    "    :param post_list: List for post-experimental phase\n",
    "    :param colonyID: colonyID corresponding to the myrmidon file\n",
    "    :return: Dataframe combining the encounters for each of the 4 phases for one colony\n",
    "    \"\"\"\n",
    "    print(\"Experimental phase\")\n",
    "    exp_enc = [\n",
    "        focal_encounters(\n",
    "            start, end, exp, focalID, exp_day, encounter_threshold, away_threshold\n",
    "        )\n",
    "        for start, end, exp, focalID, exp_day, encounter_threshold, away_threshold in exp_list\n",
    "    ]\n",
    "    exp_enc = pd.concat(exp_enc)\n",
    "    exp_enc[\"phase\"] = \"Exp\"\n",
    "    print(\"Control phase\")\n",
    "    control_enc = [\n",
    "        focal_encounters(\n",
    "            start, end, exp, focalID, exp_day, encounter_threshold, away_threshold\n",
    "        )\n",
    "        for start, end, exp, focalID, exp_day, encounter_threshold, away_threshold in control_list\n",
    "    ]\n",
    "    control_enc = pd.concat(control_enc)\n",
    "    control_enc[\"phase\"] = \"Control\"\n",
    "    print(\"Pre-Experimental phase\")\n",
    "    pre_enc = [\n",
    "        focal_encounters(\n",
    "            start, end, exp, focalID, exp_day, encounter_threshold, away_threshold\n",
    "        )\n",
    "        for start, end, exp, focalID, exp_day, encounter_threshold, away_threshold in pre_list\n",
    "    ]\n",
    "    pre_enc = pd.concat(pre_enc)\n",
    "    pre_enc[\"phase\"] = \"Pre\"\n",
    "    print(\"Post-Experimental phase\")\n",
    "    post_enc = [\n",
    "        focal_encounters(\n",
    "            start, end, exp, focalID, exp_day, encounter_threshold, away_threshold\n",
    "        )\n",
    "        for start, end, exp, focalID, exp_day, encounter_threshold, away_threshold in post_list\n",
    "    ]\n",
    "    post_enc = pd.concat(post_enc)\n",
    "    post_enc[\"phase\"] = \"Post\"\n",
    "    # Combine encounter dataframes\n",
    "    enc_list = [exp_enc, control_enc, pre_enc, post_enc]\n",
    "    enc = pd.concat(enc_list)\n",
    "    # Add colonyID\n",
    "    enc[\"colony\"] = colonyID\n",
    "    # Sort by values\n",
    "    enc = enc.sort_values(\n",
    "        by=[\"colony\", \"phase\", \"exp_day\", \"focalID\", \"AntID\"]\n",
    "    ).reset_index(drop=True)\n",
    "    enc = enc[\n",
    "        [\n",
    "            \"colony\",\n",
    "            \"phase\",\n",
    "            \"exp_day\",\n",
    "            \"focalID\",\n",
    "            \"AntID\",\n",
    "            \"enc_number\",\n",
    "            \"enc_start_time\",\n",
    "            \"enc_duration\",\n",
    "            \"enc_sequences\",\n",
    "            \"enc_sequences_duration\",\n",
    "        ]\n",
    "    ]\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Injury Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholds\n",
    "encounter_threshold = 300  # threshold for counting as an encounter\n",
    "away_threshold = 1000  # Threshold for counting as the end of an encounter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Colony Cfel 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_myrmidon = \"/media/ebiag/Ebi-2/Woundcare Experiment1/Cfell_wound_col42.myrmidon\"\n",
    "exp = fm.Experiment.Open(f_myrmidon)\n",
    "# Create list of focal ants\n",
    "focal = [106, 63, 23, 53, 19, 22, 24, 103, 94, 102]\n",
    "# Experimental phase\n",
    "day_starts_exp = [\n",
    "    datetime(2022, 5, 2, 16, 3).astimezone(tz=None),\n",
    "    datetime(2022, 5, 3, 15, 53).astimezone(tz=None),\n",
    "    datetime(2022, 5, 4, 15, 50).astimezone(tz=None),\n",
    "    datetime(2022, 5, 5, 15, 50).astimezone(tz=None),\n",
    "    datetime(2022, 5, 6, 15, 55).astimezone(tz=None),\n",
    "] * 2\n",
    "day_ends_exp = [day_time + timedelta(hours=6) for day_time in day_starts_exp]\n",
    "exp_days = [1, 2, 3, 4, 5] * 2\n",
    "\n",
    "disp_list_exp = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(day_starts_exp, day_ends_exp, focal, exp_days)\n",
    "]\n",
    "\n",
    "# Control Phase\n",
    "day_starts_control = list(\n",
    "    np.repeat(datetime(2022, 5, 1, 15, 54).astimezone(tz=None), 10)\n",
    ")\n",
    "day_ends_control = [day_time + timedelta(hours=6) for day_time in day_starts_control]\n",
    "\n",
    "disp_list_control = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(\n",
    "        day_starts_control, day_ends_control, focal, exp_days\n",
    "    )\n",
    "]\n",
    "\n",
    "# Pre experimental phase\n",
    "day_starts_pre = [\n",
    "    datetime(2022, 5, 2, 9, 0).astimezone(tz=None),\n",
    "    datetime(2022, 5, 3, 9, 0).astimezone(tz=None),\n",
    "    datetime(2022, 5, 4, 9, 0).astimezone(tz=None),\n",
    "    datetime(2022, 5, 5, 9, 0).astimezone(tz=None),\n",
    "    datetime(2022, 5, 6, 9, 0).astimezone(tz=None),\n",
    "] * 2\n",
    "day_ends_pre = [day_time + timedelta(hours=6) for day_time in day_starts_pre]\n",
    "\n",
    "disp_list_pre = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(day_starts_pre, day_ends_pre, focal, exp_days)\n",
    "]\n",
    "\n",
    "# Post experimental phase\n",
    "day_starts_post = [\n",
    "    datetime(2022, 5, 3, 9, 0).astimezone(tz=None),\n",
    "    datetime(2022, 5, 4, 9, 0).astimezone(tz=None),\n",
    "    datetime(2022, 5, 5, 9, 0).astimezone(tz=None),\n",
    "    datetime(2022, 5, 6, 9, 0).astimezone(tz=None),\n",
    "    datetime(2022, 5, 7, 9, 0).astimezone(tz=None),\n",
    "] * 2\n",
    "day_ends_post = [day_time + timedelta(hours=6) for day_time in day_starts_post]\n",
    "\n",
    "disp_list_post = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(\n",
    "        day_starts_post, day_ends_post, focal, exp_days\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encounters for experimental day 1 calculated in 0:02:44.439539\n",
      "Encounters for experimental day 2 calculated in 0:04:45.928922\n",
      "Encounters for experimental day 3 calculated in 0:02:00.413341\n",
      "Encounters for experimental day 4 calculated in 0:02:07.269116\n",
      "Encounters for experimental day 5 calculated in 0:02:14.014184\n",
      "Encounters for experimental day 1 calculated in 0:03:35.573845\n",
      "Encounters for experimental day 2 calculated in 0:02:42.963323\n",
      "Encounters for experimental day 3 calculated in 0:03:48.218443\n",
      "Encounters for experimental day 4 calculated in 0:00:57.718380\n",
      "Encounters for experimental day 5 calculated in 0:02:08.735015\n"
     ]
    }
   ],
   "source": [
    "exp_enc = [\n",
    "    focal_encounters(\n",
    "        start, end, exp, focalID, exp_day, encounter_threshold, away_threshold\n",
    "    )\n",
    "    for start, end, exp, focalID, exp_day, encounter_threshold, away_threshold in disp_list_exp\n",
    "]\n",
    "exp_enc = pd.concat(exp_enc)\n",
    "exp_enc[\"phase\"] = \"Exp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encounters for experimental day 1 calculated in 0:04:09.546647\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/fort/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniforge3/envs/fort/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/fort/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 5222611",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/fort/lib/python3.8/site-packages/pandas/core/indexes/base.py:6456\u001b[0m, in \u001b[0;36mIndex.get_slice_bound\u001b[0;34m(self, label, side)\u001b[0m\n\u001b[1;32m   6455\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 6456\u001b[0m     slc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6457\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniforge3/envs/fort/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 5222611",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ctrl_enc \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     focal_encounters(\n\u001b[1;32m      3\u001b[0m         start, end, exp, focalID, exp_day, encounter_threshold, away_threshold\n\u001b[1;32m      4\u001b[0m     )\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m start, end, exp, focalID, exp_day, encounter_threshold, away_threshold \u001b[38;5;129;01min\u001b[39;00m disp_list_control\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      7\u001b[0m ctrl_enc \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(ctrl_enc)\n\u001b[1;32m      8\u001b[0m ctrl_enc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphase\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mControl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m ctrl_enc \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mfocal_encounters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfocalID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_day\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencounter_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maway_threshold\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m start, end, exp, focalID, exp_day, encounter_threshold, away_threshold \u001b[38;5;129;01min\u001b[39;00m disp_list_control\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      7\u001b[0m ctrl_enc \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(ctrl_enc)\n\u001b[1;32m      8\u001b[0m ctrl_enc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphase\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mControl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 136\u001b[0m, in \u001b[0;36mfocal_encounters\u001b[0;34m(start_time, end_time, exp, focal_ID, exp_day, encounter_threshold, away_threshold)\u001b[0m\n\u001b[1;32m    126\u001b[0m full_traj\u001b[38;5;241m.\u001b[39mloc[\n\u001b[1;32m    127\u001b[0m     (\n\u001b[1;32m    128\u001b[0m         (full_traj\u001b[38;5;241m.\u001b[39mSpace_focal\u001b[38;5;241m.\u001b[39mnotnull())\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    133\u001b[0m ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Apply encounter_duration function over grouped dataframe, reset index and rename columns\u001b[39;00m\n\u001b[1;32m    135\u001b[0m enc_df \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 136\u001b[0m     \u001b[43mfull_traj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexp_day\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfocalID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAntID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mencounter_duration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencounter_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maway_threshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel_3\u001b[39m\u001b[38;5;124m\"\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    141\u001b[0m end \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEncounters for experimental day \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexp_day\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m calculated in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/fort/lib/python3.8/site-packages/pandas/core/groupby/groupby.py:1353\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1352\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1353\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1354\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1355\u001b[0m         \u001b[38;5;66;03m# gh-20949\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m         \u001b[38;5;66;03m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[38;5;66;03m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m         \u001b[38;5;66;03m# on a string grouper column\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions)\n",
      "File \u001b[0;32m~/miniforge3/envs/fort/lib/python3.8/site-packages/pandas/core/groupby/groupby.py:1402\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[0;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1374\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1375\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[1;32m   1378\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1402\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1404\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[0;32m~/miniforge3/envs/fort/lib/python3.8/site-packages/pandas/core/groupby/ops.py:767\u001b[0m, in \u001b[0;36mBaseGrouper.apply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[1;32m    766\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[0;32m--> 767\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[1;32m    769\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 137\u001b[0m, in \u001b[0;36mfocal_encounters.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    126\u001b[0m full_traj\u001b[38;5;241m.\u001b[39mloc[\n\u001b[1;32m    127\u001b[0m     (\n\u001b[1;32m    128\u001b[0m         (full_traj\u001b[38;5;241m.\u001b[39mSpace_focal\u001b[38;5;241m.\u001b[39mnotnull())\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    133\u001b[0m ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Apply encounter_duration function over grouped dataframe, reset index and rename columns\u001b[39;00m\n\u001b[1;32m    135\u001b[0m enc_df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    136\u001b[0m     full_traj\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_day\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfocalID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAntID\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mencounter_duration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencounter_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maway_threshold\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel_3\u001b[39m\u001b[38;5;124m\"\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    141\u001b[0m end \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEncounters for experimental day \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexp_day\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m calculated in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m )\n",
      "Cell \u001b[0;32mIn[17], line 64\u001b[0m, in \u001b[0;36mencounter_duration\u001b[0;34m(displacement_dataset, encounter_threshold, away_threshold)\u001b[0m\n\u001b[1;32m     62\u001b[0m away_indices_pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(away_indices, away_indices[\u001b[38;5;241m1\u001b[39m:]))\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Get values between the consecutive indices with value of 1\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m seq_bw_away \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     65\u001b[0m     displacement_dataset\u001b[38;5;241m.\u001b[39mloc[x \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m : y \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menc_dummy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (x, y) \u001b[38;5;129;01min\u001b[39;00m away_indices_pair\n\u001b[1;32m     67\u001b[0m ]\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Remove list elements which have only 1 index value\u001b[39;00m\n\u001b[1;32m     69\u001b[0m seq_bw_away_sub \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m seq_bw_away \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[17], line 65\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     62\u001b[0m away_indices_pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(away_indices, away_indices[\u001b[38;5;241m1\u001b[39m:]))\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Get values between the consecutive indices with value of 1\u001b[39;00m\n\u001b[1;32m     64\u001b[0m seq_bw_away \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 65\u001b[0m     \u001b[43mdisplacement_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menc_dummy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (x, y) \u001b[38;5;129;01min\u001b[39;00m away_indices_pair\n\u001b[1;32m     67\u001b[0m ]\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Remove list elements which have only 1 index value\u001b[39;00m\n\u001b[1;32m     69\u001b[0m seq_bw_away_sub \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m seq_bw_away \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/fort/lib/python3.8/site-packages/pandas/core/indexing.py:1097\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1096\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[0;32m-> 1097\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/fort/lib/python3.8/site-packages/pandas/core/indexing.py:1280\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[1;32m   1279\u001b[0m     tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_ellipsis(tup)\n\u001b[0;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_lowerdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;66;03m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tuple_indexer(tup)\n",
      "File \u001b[0;32m~/miniforge3/envs/fort/lib/python3.8/site-packages/pandas/core/indexing.py:1024\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m section\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;66;03m# This is an elided recursive call to iloc/loc\u001b[39;00m\n\u001b[0;32m-> 1024\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m IndexingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot applicable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/fort/lib/python3.8/site-packages/pandas/core/indexing.py:1103\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1100\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1102\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/fort/lib/python3.8/site-packages/pandas/core/indexing.py:1323\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m-> 1323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_slice_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getbool_axis(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/miniforge3/envs/fort/lib/python3.8/site-packages/pandas/core/indexing.py:1355\u001b[0m, in \u001b[0;36m_LocIndexer._get_slice_axis\u001b[0;34m(self, slice_obj, axis)\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1354\u001b[0m labels \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[0;32m-> 1355\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m   1358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_slice(indexer, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/miniforge3/envs/fort/lib/python3.8/site-packages/pandas/core/indexes/base.py:6344\u001b[0m, in \u001b[0;36mIndex.slice_indexer\u001b[0;34m(self, start, end, step)\u001b[0m\n\u001b[1;32m   6300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mslice_indexer\u001b[39m(\n\u001b[1;32m   6301\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   6302\u001b[0m     start: Hashable \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   6303\u001b[0m     end: Hashable \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   6304\u001b[0m     step: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   6305\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mslice\u001b[39m:\n\u001b[1;32m   6306\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6307\u001b[0m \u001b[38;5;124;03m    Compute the slice indexer for input labels and step.\u001b[39;00m\n\u001b[1;32m   6308\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6342\u001b[0m \u001b[38;5;124;03m    slice(1, 3, None)\u001b[39;00m\n\u001b[1;32m   6343\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 6344\u001b[0m     start_slice, end_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice_locs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6346\u001b[0m     \u001b[38;5;66;03m# return a slice\u001b[39;00m\n\u001b[1;32m   6347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(start_slice):\n",
      "File \u001b[0;32m~/miniforge3/envs/fort/lib/python3.8/site-packages/pandas/core/indexes/base.py:6537\u001b[0m, in \u001b[0;36mIndex.slice_locs\u001b[0;34m(self, start, end, step)\u001b[0m\n\u001b[1;32m   6535\u001b[0m start_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   6536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 6537\u001b[0m     start_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_slice_bound\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_slice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6539\u001b[0m     start_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/fort/lib/python3.8/site-packages/pandas/core/indexes/base.py:6459\u001b[0m, in \u001b[0;36mIndex.get_slice_bound\u001b[0;34m(self, label, side)\u001b[0m\n\u001b[1;32m   6457\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   6458\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 6459\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_searchsorted_monotonic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6460\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   6461\u001b[0m         \u001b[38;5;66;03m# raise the original KeyError\u001b[39;00m\n\u001b[1;32m   6462\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[0;32m~/miniforge3/envs/fort/lib/python3.8/site-packages/pandas/core/indexes/base.py:6412\u001b[0m, in \u001b[0;36mIndex._searchsorted_monotonic\u001b[0;34m(self, label, side)\u001b[0m\n\u001b[1;32m   6410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_searchsorted_monotonic\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, side: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   6411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_monotonic_increasing:\n\u001b[0;32m-> 6412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearchsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6413\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_monotonic_decreasing:\n\u001b[1;32m   6414\u001b[0m         \u001b[38;5;66;03m# np.searchsorted expects ascending sort order, have to reverse\u001b[39;00m\n\u001b[1;32m   6415\u001b[0m         \u001b[38;5;66;03m# everything for it to work (element ordering, search side and\u001b[39;00m\n\u001b[1;32m   6416\u001b[0m         \u001b[38;5;66;03m# resulting value).\u001b[39;00m\n\u001b[1;32m   6417\u001b[0m         pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msearchsorted(\n\u001b[1;32m   6418\u001b[0m             label, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m side \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6419\u001b[0m         )\n",
      "File \u001b[0;32m~/miniforge3/envs/fort/lib/python3.8/site-packages/pandas/core/base.py:1323\u001b[0m, in \u001b[0;36mIndexOpsMixin.searchsorted\u001b[0;34m(self, value, side, sorter)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;66;03m# Going through EA.searchsorted directly improves performance GH#38083\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\u001b[38;5;241m.\u001b[39msearchsorted(value, side\u001b[38;5;241m=\u001b[39mside, sorter\u001b[38;5;241m=\u001b[39msorter)\n\u001b[0;32m-> 1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearchsorted\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43msorter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/fort/lib/python3.8/site-packages/pandas/core/algorithms.py:1327\u001b[0m, in \u001b[0;36msearchsorted\u001b[0;34m(arr, value, side, sorter)\u001b[0m\n\u001b[1;32m   1316\u001b[0m     sorter \u001b[38;5;241m=\u001b[39m ensure_platform_int(sorter)\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(arr, np\u001b[38;5;241m.\u001b[39mndarray)\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_integer_dtype(arr\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;66;03m# Before searching below, we therefore try to give `value` the\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;66;03m# same dtype as `arr`, while guarding against integer overflows.\u001b[39;00m\n\u001b[0;32m-> 1327\u001b[0m     iinfo \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m     value_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([value]) \u001b[38;5;28;01mif\u001b[39;00m is_scalar(value) \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(value)\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (value_arr \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m iinfo\u001b[38;5;241m.\u001b[39mmin)\u001b[38;5;241m.\u001b[39mall() \u001b[38;5;129;01mand\u001b[39;00m (value_arr \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m iinfo\u001b[38;5;241m.\u001b[39mmax)\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m   1330\u001b[0m         \u001b[38;5;66;03m# value within bounds, so no overflow, so can convert value dtype\u001b[39;00m\n\u001b[1;32m   1331\u001b[0m         \u001b[38;5;66;03m# to dtype of arr\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ctrl_enc = [\n",
    "    focal_encounters(\n",
    "        start, end, exp, focalID, exp_day, encounter_threshold, away_threshold\n",
    "    )\n",
    "    for start, end, exp, focalID, exp_day, encounter_threshold, away_threshold in disp_list_control\n",
    "]\n",
    "ctrl_enc = pd.concat(ctrl_enc)\n",
    "ctrl_enc[\"phase\"] = \"Control\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_enc = [\n",
    "    focal_encounters(\n",
    "        start, end, exp, focalID, exp_day, encounter_threshold, away_threshold\n",
    "    )\n",
    "    for start, end, exp, focalID, exp_day, encounter_threshold, away_threshold in disp_list_pre\n",
    "]\n",
    "pre_enc = pd.concat(pre_enc)\n",
    "pre_enc[\"phase\"] = \"Pre\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_enc = [\n",
    "    focal_encounters(\n",
    "        start, end, exp, focalID, exp_day, encounter_threshold, away_threshold\n",
    "    )\n",
    "    for start, end, exp, focalID, exp_day, encounter_threshold, away_threshold in disp_list_post\n",
    "]\n",
    "post_enc = pd.concat(post_enc)\n",
    "post_enc[\"phase\"] = \"Post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colonyID = \"Cfel42\"\n",
    "# Combine encounter dataframes\n",
    "enc_list = [exp_enc, ctrl_enc, pre_enc, post_enc]\n",
    "enc = pd.concat(enc_list)\n",
    "# Add colonyID\n",
    "enc[\"colony\"] = colonyID\n",
    "# Sort by values\n",
    "enc = enc.sort_values(\n",
    "    by=[\"colony\", \"phase\", \"exp_day\", \"focalID\", \"AntID\"]\n",
    ").reset_index(drop=True)\n",
    "enc = enc[\n",
    "    [\n",
    "        \"colony\",\n",
    "        \"phase\",\n",
    "        \"exp_day\",\n",
    "        \"focalID\",\n",
    "        \"AntID\",\n",
    "        \"enc_number\",\n",
    "        \"enc_start_time\",\n",
    "        \"enc_duration\",\n",
    "        \"enc_sequences\",\n",
    "        \"enc_sequences_duration\",\n",
    "    ]\n",
    "]\n",
    "# Save to CSV\n",
    "enc.to_csv(\"Cfel42_AllAnts_Focal_Encounters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimental phase\n",
      "Encounters for experimental day 1 calculated in 0:03:13.661084\n",
      "Encounters for experimental day 2 calculated in 0:05:28.910801\n",
      "Encounters for experimental day 3 calculated in 0:02:39.460039\n",
      "Encounters for experimental day 4 calculated in 0:02:57.221570\n",
      "Encounters for experimental day 5 calculated in 0:03:14.032626\n",
      "Encounters for experimental day 1 calculated in 0:05:36.792071\n",
      "Encounters for experimental day 2 calculated in 0:05:04.176355\n",
      "Encounters for experimental day 3 calculated in 0:08:20.989096\n",
      "Encounters for experimental day 4 calculated in 0:03:00.067046\n",
      "Encounters for experimental day 5 calculated in 0:08:58.424003\n",
      "Control phase\n"
     ]
    }
   ],
   "source": [
    "cfel42_enc = calculate_encounters_cluster(\n",
    "    disp_list_exp, disp_list_control, disp_list_pre, disp_list_post, \"Cfel42\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfel42_enc.to_csv(\"Cfel42_AllAnts_Focal_Encounters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Colony Cfel 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_myrmidon = \"/media/ebiag/Ebi-2/Woundcare Experiment2/woundcare_cfell1_T2.myrmidon\"\n",
    "exp = fm.Experiment.Open(f_myrmidon)\n",
    "# Focal Ants\n",
    "focal = [87, 37, 58, 38, 3, 67, 2, 46, 30, 54]\n",
    "exp_days = [1, 2, 3, 4, 5] * 2\n",
    "# Experimental Phase list\n",
    "day_starts_exp = [\n",
    "    datetime(2022, 6, 5, 14, 57).astimezone(tz=None),\n",
    "    datetime(2022, 6, 6, 14, 30).astimezone(tz=None),\n",
    "    datetime(2022, 6, 7, 14, 49).astimezone(tz=None),\n",
    "    datetime(2022, 6, 8, 14, 43).astimezone(tz=None),\n",
    "    datetime(2022, 6, 9, 15, 5).astimezone(tz=None),\n",
    "] * 2\n",
    "day_ends_exp = [day_time + timedelta(hours=6) for day_time in day_starts_exp]\n",
    "\n",
    "disp_list_exp = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(day_starts_exp, day_ends_exp, focal, exp_days)\n",
    "]\n",
    "\n",
    "# Control phase list\n",
    "day_starts_control = list(\n",
    "    np.repeat(datetime(2022, 6, 4, 14, 48).astimezone(tz=None), 10)\n",
    ")\n",
    "day_ends_control = [day_time + timedelta(hours=6) for day_time in day_starts_control]\n",
    "\n",
    "disp_list_control = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(\n",
    "        day_starts_control, day_ends_control, focal, exp_days\n",
    "    )\n",
    "]\n",
    "\n",
    "# Pre Experimental phase list\n",
    "day_starts_pre = [\n",
    "    datetime(2022, 6, 5, 8, 0).astimezone(tz=None),\n",
    "    datetime(2022, 6, 6, 8, 0).astimezone(tz=None),\n",
    "    datetime(2022, 6, 7, 8, 0).astimezone(tz=None),\n",
    "    datetime(2022, 6, 8, 8, 0).astimezone(tz=None),\n",
    "    datetime(2022, 6, 9, 8, 0).astimezone(tz=None),\n",
    "] * 2\n",
    "day_ends_pre = [day_time + timedelta(hours=6) for day_time in day_starts_pre]\n",
    "\n",
    "disp_list_pre = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(day_starts_pre, day_ends_pre, focal, exp_days)\n",
    "]\n",
    "\n",
    "# Post experimental phase list\n",
    "day_starts_post = [\n",
    "    datetime(2022, 6, 6, 8, 0).astimezone(tz=None),\n",
    "    datetime(2022, 6, 7, 8, 0).astimezone(tz=None),\n",
    "    datetime(2022, 6, 8, 8, 0).astimezone(tz=None),\n",
    "    datetime(2022, 6, 9, 8, 0).astimezone(tz=None),\n",
    "    datetime(2022, 6, 10, 8, 0).astimezone(tz=None),\n",
    "] * 2\n",
    "day_ends_post = [day_time + timedelta(hours=6) for day_time in day_starts_post]\n",
    "\n",
    "disp_list_post = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(\n",
    "        day_starts_post, day_ends_post, focal, exp_days\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfel1_enc = calculate_encounters_cluster(\n",
    "    disp_list_exp, disp_list_control, disp_list_pre, disp_list_post, \"Cfel1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfel1_enc.to_csv(\"Cfel1_AllAnts_Focal_Encounters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Colony Cfel 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_myrmidon = \"/media/ebiag/Ebi-2/Woundcare Experiment3/woundcare_cfell54_T3.myrmidon\"\n",
    "exp = fm.Experiment.Open(f_myrmidon)\n",
    "# Focal Ants\n",
    "focal = [108, 114, 62, 12, 53, 107, 9, 87, 83, 101]\n",
    "exp_days = [1, 2, 3, 4, 5] * 2\n",
    "# Experimental Phase list\n",
    "day_starts_exp = [\n",
    "    datetime(2022, 6, 20, 14, 35).astimezone(tz=None),\n",
    "    datetime(2022, 6, 21, 14, 21).astimezone(tz=None),\n",
    "    datetime(2022, 6, 22, 14, 28).astimezone(tz=None),\n",
    "    datetime(2022, 6, 23, 14, 14).astimezone(tz=None),\n",
    "    datetime(2022, 6, 24, 14, 31).astimezone(tz=None),\n",
    "]\n",
    "day_ends_exp = [day_time + timedelta(hours=6) for day_time in day_starts_exp]\n",
    "\n",
    "disp_list_exp = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(day_starts_exp, day_ends_exp, focal, exp_days)\n",
    "]\n",
    "\n",
    "# Control phase list\n",
    "day_starts_control = list(\n",
    "    np.repeat(datetime(2022, 6, 19, 14, 25).astimezone(tz=None), 10)\n",
    ")\n",
    "day_ends_control = [day_time + timedelta(hours=6) for day_time in day_starts_control]\n",
    "\n",
    "disp_list_control = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(\n",
    "        day_starts_control, day_ends_control, focal, exp_days\n",
    "    )\n",
    "]\n",
    "\n",
    "# Pre Experimental phase list\n",
    "day_starts_pre = [\n",
    "    datetime(2022, 6, 20, 8, 0).astimezone(tz=None),\n",
    "    datetime(2022, 6, 21, 8, 0).astimezone(tz=None),\n",
    "    datetime(2022, 6, 22, 8, 0).astimezone(tz=None),\n",
    "    datetime(2022, 6, 23, 8, 0).astimezone(tz=None),\n",
    "    datetime(2022, 6, 24, 8, 0).astimezone(tz=None),\n",
    "] * 2\n",
    "day_ends_pre = [day_time + timedelta(hours=6) for day_time in day_starts_pre]\n",
    "\n",
    "disp_list_pre = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(day_starts_pre, day_ends_pre, focal, exp_days)\n",
    "]\n",
    "\n",
    "# Post experimental phase list\n",
    "day_starts_post = [\n",
    "    datetime(2022, 6, 21, 8, 0).astimezone(tz=None),\n",
    "    datetime(2022, 6, 22, 8, 0).astimezone(tz=None),\n",
    "    datetime(2022, 6, 23, 8, 0).astimezone(tz=None),\n",
    "    datetime(2022, 6, 24, 8, 0).astimezone(tz=None),\n",
    "    datetime(2022, 6, 25, 8, 0).astimezone(tz=None),\n",
    "] * 2\n",
    "day_ends_post = [day_time + timedelta(hours=6) for day_time in day_starts_post]\n",
    "\n",
    "disp_list_post = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(\n",
    "        day_starts_post, day_ends_post, focal, exp_days\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfel54_enc = calculate_encounters_cluster(\n",
    "    disp_list_exp, disp_list_control, disp_list_pre, disp_list_post, \"Cfel54\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfel54_enc.to_csv(\"Cfel54_AllAnts_Focal_Encounters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infection Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colony Cfel 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_myrmidon = \"/media/ebiag/Ebi-3/InfectionExp_Cfel13/InfectionExp_Cfel13.myrmidon\"\n",
    "exp = fm.Experiment.Open(f_myrmidon)\n",
    "# Focal Ants\n",
    "focal = [9, 82, 40, 7, 55, 80, 26, 22, 27, 98]\n",
    "exp_days = [1, 2, 3, 4, 5] * 2\n",
    "# Experimental Phase list\n",
    "day_starts_exp = [\n",
    "    datetime(2023, 4, 24, 15, 29).astimezone(tz=None),\n",
    "    datetime(2023, 4, 25, 14, 19).astimezone(tz=None),\n",
    "    datetime(2023, 4, 26, 15, 3).astimezone(tz=None),\n",
    "    datetime(2023, 4, 27, 16, 43).astimezone(tz=None),\n",
    "    datetime(2023, 4, 28, 14, 27).astimezone(tz=None),\n",
    "] * 2\n",
    "day_ends_exp = [day_time + timedelta(hours=6) for day_time in day_starts_exp]\n",
    "\n",
    "disp_list_exp = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(day_starts_exp, day_ends_exp, focal, exp_days)\n",
    "]\n",
    "\n",
    "# Control phase list\n",
    "day_starts_control = list(\n",
    "    np.repeat(datetime(2023, 4, 23, 15, 5).astimezone(tz=None), 10)\n",
    ")\n",
    "day_ends_control = [day_time + timedelta(hours=6) for day_time in day_starts_control]\n",
    "\n",
    "disp_list_control = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(\n",
    "        day_starts_control, day_ends_control, focal, exp_days\n",
    "    )\n",
    "]\n",
    "\n",
    "# Pre Experimental phase list\n",
    "day_starts_pre = [\n",
    "    datetime(2023, 4, 24, 8, 0).astimezone(tz=None),\n",
    "    datetime(2023, 4, 25, 8, 0).astimezone(tz=None),\n",
    "    datetime(2023, 4, 26, 8, 0).astimezone(tz=None),\n",
    "    datetime(2023, 4, 27, 8, 0).astimezone(tz=None),\n",
    "    datetime(2023, 4, 28, 8, 0).astimezone(tz=None),\n",
    "] * 2\n",
    "day_ends_pre = [day_time + timedelta(hours=6) for day_time in day_starts_pre]\n",
    "\n",
    "disp_list_pre = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(day_starts_pre, day_ends_pre, focal, exp_days)\n",
    "]\n",
    "\n",
    "# Post experimental phase list\n",
    "day_starts_post = [\n",
    "    datetime(2023, 4, 25, 8, 0).astimezone(tz=None),\n",
    "    datetime(2023, 4, 26, 8, 0).astimezone(tz=None),\n",
    "    datetime(2023, 4, 27, 8, 0).astimezone(tz=None),\n",
    "    datetime(2023, 4, 28, 8, 0).astimezone(tz=None),\n",
    "    datetime(2023, 4, 29, 8, 0).astimezone(tz=None),\n",
    "] * 2\n",
    "day_ends_post = [day_time + timedelta(hours=6) for day_time in day_starts_post]\n",
    "\n",
    "disp_list_post = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(\n",
    "        day_starts_post, day_ends_post, focal, exp_days\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfel13_enc = calculate_encounters_cluster(\n",
    "    disp_list_exp, disp_list_control, disp_list_pre, disp_list_post, \"Cfel13\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfel13_enc.to_csv(\"Cfel13_AllAnts_Focal_Encounters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colony Cfel 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_myrmidon = \"/media/ebiag/Ebi-3/InfectionExp_Cfel55/InfectionExpCol55.myrmidon\"\n",
    "exp = fm.Experiment.Open(f_myrmidon)\n",
    "# Focal Ants\n",
    "focal = [30, 36, 44, 53, 55, 72, 15, 57, 67, 81]\n",
    "exp_days = [1, 2, 3, 4, 5] * 2\n",
    "# Experimental Phase list\n",
    "day_starts_exp = [\n",
    "    datetime(2023, 4, 20, 15, 45).astimezone(tz=None),\n",
    "    datetime(2023, 4, 21, 14, 48).astimezone(tz=None),\n",
    "    datetime(2023, 4, 22, 14, 17).astimezone(tz=None),\n",
    "    datetime(2023, 4, 23, 14, 0).astimezone(tz=None),\n",
    "    datetime(2023, 4, 24, 14, 54).astimezone(tz=None),\n",
    "] * 2\n",
    "day_ends_exp = [day_time + timedelta(hours=6) for day_time in day_starts_exp]\n",
    "\n",
    "disp_list_exp = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(day_starts_exp, day_ends_exp, focal, exp_days)\n",
    "]\n",
    "\n",
    "# Control phase list\n",
    "day_starts_control = list(\n",
    "    np.repeat(datetime(2023, 4, 18, 14, 40).astimezone(tz=None), 10)\n",
    ")\n",
    "day_ends_control = [day_time + timedelta(hours=6) for day_time in day_starts_control]\n",
    "\n",
    "disp_list_control = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(\n",
    "        day_starts_control, day_ends_control, focal, exp_days\n",
    "    )\n",
    "]\n",
    "\n",
    "# Pre Experimental phase list\n",
    "day_starts_pre = [\n",
    "    datetime(2023, 4, 20, 8, 0).astimezone(tz=None),\n",
    "    datetime(2023, 4, 21, 8, 0).astimezone(tz=None),\n",
    "    datetime(2023, 4, 22, 7, 30).astimezone(tz=None),\n",
    "    datetime(2023, 4, 23, 7, 30).astimezone(tz=None),\n",
    "    datetime(2023, 4, 24, 8, 0).astimezone(tz=None),\n",
    "] * 2\n",
    "day_ends_pre = [day_time + timedelta(hours=6) for day_time in day_starts_pre]\n",
    "\n",
    "disp_list_pre = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(day_starts_pre, day_ends_pre, focal, exp_days)\n",
    "]\n",
    "\n",
    "# Post experimental phase list\n",
    "day_starts_post = [\n",
    "    datetime(2023, 4, 21, 8, 0).astimezone(tz=None),\n",
    "    datetime(2023, 4, 22, 7, 30).astimezone(tz=None),\n",
    "    datetime(2023, 4, 23, 7, 30).astimezone(tz=None),\n",
    "    datetime(2023, 4, 24, 8, 0).astimezone(tz=None),\n",
    "    datetime(2023, 4, 25, 8, 0).astimezone(tz=None),\n",
    "] * 2\n",
    "day_ends_post = [day_time + timedelta(hours=6) for day_time in day_starts_post]\n",
    "\n",
    "disp_list_post = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(\n",
    "        day_starts_post, day_ends_post, focal, exp_days\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfel55_enc = calculate_encounters_cluster(\n",
    "    disp_list_exp, disp_list_control, disp_list_pre, disp_list_post, \"Cfel55\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfel55_enc.to_csv(\"Cfel55_AllAnts_Focal_Encounters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colony Cfel 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_myrmidon = \"/media/ebiag/Ebi-1/InfectionExp_Cfel64/InfectionExpCol64.myrmidon\"\n",
    "exp = fm.Experiment.Open(f_myrmidon)\n",
    "# Focal Ants\n",
    "focal = [6, 104, 115, 78, 59, 32, 86, 1, 38, 3]\n",
    "exp_days = [1, 2, 3, 4, 5] * 2\n",
    "# Experimental Phase list\n",
    "day_starts_exp = [\n",
    "    datetime(2023, 6, 1, 15, 51).astimezone(tz=None),\n",
    "    datetime(2023, 6, 2, 14, 44).astimezone(tz=None),\n",
    "    datetime(2023, 6, 3, 14, 50).astimezone(tz=None),\n",
    "    datetime(2023, 6, 4, 14, 43).astimezone(tz=None),\n",
    "    datetime(2023, 6, 5, 14, 52).astimezone(tz=None),\n",
    "] * 2\n",
    "day_ends_exp = [day_time + timedelta(hours=6) for day_time in day_starts_exp]\n",
    "\n",
    "disp_list_exp = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(day_starts_exp, day_ends_exp, focal, exp_days)\n",
    "]\n",
    "\n",
    "# Control phase list\n",
    "day_starts_control = list(\n",
    "    np.repeat(datetime(2023, 5, 31, 15, 5).astimezone(tz=None), 10)\n",
    ")\n",
    "day_ends_control = [day_time + timedelta(hours=6) for day_time in day_starts_control]\n",
    "\n",
    "disp_list_control = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(\n",
    "        day_starts_control, day_ends_control, focal, exp_days\n",
    "    )\n",
    "]\n",
    "\n",
    "# Pre Experimental phase list\n",
    "day_starts_pre = [\n",
    "    datetime(2023, 6, 1, 8, 0).astimezone(tz=None),\n",
    "    datetime(2023, 6, 2, 8, 0).astimezone(tz=None),\n",
    "    datetime(2023, 6, 3, 8, 0).astimezone(tz=None),\n",
    "    datetime(2023, 6, 4, 8, 0).astimezone(tz=None),\n",
    "    datetime(2023, 6, 5, 8, 0).astimezone(tz=None),\n",
    "] * 2\n",
    "day_ends_pre = [day_time + timedelta(hours=6) for day_time in day_starts_pre]\n",
    "\n",
    "disp_list_pre = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(day_starts_pre, day_ends_pre, focal, exp_days)\n",
    "]\n",
    "\n",
    "# Post experimental phase list\n",
    "day_starts_post = [\n",
    "    datetime(2023, 6, 2, 8, 0).astimezone(tz=None),\n",
    "    datetime(2023, 6, 3, 8, 0).astimezone(tz=None),\n",
    "    datetime(2023, 6, 4, 8, 0).astimezone(tz=None),\n",
    "    datetime(2023, 6, 5, 8, 0).astimezone(tz=None),\n",
    "    datetime(2023, 6, 6, 8, 0).astimezone(tz=None),\n",
    "] * 2\n",
    "day_ends_post = [day_time + timedelta(hours=6) for day_time in day_starts_post]\n",
    "\n",
    "disp_list_post = [\n",
    "    (start, end, exp, focal, exp_day, encounter_threshold, away_threshold)\n",
    "    for start, end, focal, exp_day in zip(\n",
    "        day_starts_post, day_ends_post, focal, exp_days\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfel64_enc = calculate_encounters_cluster(\n",
    "    disp_list_exp, disp_list_control, disp_list_pre, disp_list_post, \"Cfel64\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfel64_enc.to_csv(\"Cfel64_AllAnts_Focal_Encounters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimised code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encounter_duration_optimized(\n",
    "    displacement_dataset, encounter_threshold, away_threshold\n",
    "):\n",
    "    \"\"\"Function to calculate duration of encounters between a focal ant and another individual.\n",
    "    Optimized version with faster NumPy operations and reduced overhead.\n",
    "\n",
    "    Args:\n",
    "        displacement_dataset (pandas.DataFrame): A pandas dataframe containing at least Timestamps and displacement between 2 ants at each timestamp.\n",
    "        encounter_threshold (int): The value of displacement for the encounter threshold\n",
    "        away_threshold (int): The value of displacement for the away threshold\n",
    "\n",
    "    Returns:\n",
    "        enc_df(pandas.DataFrame): A dataframe containing encounter metrics\n",
    "    \"\"\"\n",
    "    # Quick check for minimum dataset size\n",
    "    if len(displacement_dataset) < 3:\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"enc_number\": [0],\n",
    "                \"enc_start_time\": [np.nan],\n",
    "                \"enc_duration\": [0.0],\n",
    "                \"enc_sequences\": [0],\n",
    "                \"enc_sequences_duration\": [0.0],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Work with copies of only the columns we need\n",
    "    disp_values = displacement_dataset[\"disp\"].values\n",
    "    # time_values = displacement_dataset[\"Time\"].values\n",
    "    index_values = displacement_dataset.index.values\n",
    "\n",
    "    # Interpolate missing values only if there are NaNs\n",
    "    if np.isnan(disp_values).any():\n",
    "        valid_indices = ~np.isnan(disp_values)\n",
    "        if np.any(valid_indices):  # Make sure there's at least one valid value\n",
    "            valid_idx = np.where(valid_indices)[0]\n",
    "            valid_disp = disp_values[valid_indices]\n",
    "            # Create interpolator function\n",
    "            interp_indices = np.arange(len(disp_values))\n",
    "            disp_values = np.interp(interp_indices, valid_idx, valid_disp)\n",
    "\n",
    "    # Create encounter dummy values using NumPy's faster conditional selection\n",
    "    # 1.0: within encounter threshold, 0.5: between encounter and away, 0.0: beyond away threshold\n",
    "    enc_dummy = np.select(\n",
    "        [\n",
    "            disp_values <= encounter_threshold,\n",
    "            (disp_values > encounter_threshold) & (disp_values <= away_threshold),\n",
    "        ],\n",
    "        [1.0, 0.5],\n",
    "        default=0.0,\n",
    "    )\n",
    "\n",
    "    # Early check if no encounters exist\n",
    "    if 1.0 not in enc_dummy:\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"enc_number\": [0],\n",
    "                \"enc_start_time\": [np.nan],\n",
    "                \"enc_duration\": [0.0],\n",
    "                \"enc_sequences\": [0],\n",
    "                \"enc_sequences_duration\": [0.0],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Find indices where ants are beyond away threshold (dummy=0)\n",
    "    away_indices = np.where(enc_dummy == 0.0)[0]\n",
    "\n",
    "    # If no away indices or too few, handle edge case\n",
    "    if len(away_indices) < 2:\n",
    "        if len(away_indices) == 0 and 1.0 in enc_dummy:\n",
    "            # Special case: entire sequence might be an encounter without an \"away\" period\n",
    "            # Find first occurrence of value 1\n",
    "            first_encounter = np.where(enc_dummy == 1.0)[0][0]\n",
    "            start_time = displacement_dataset.loc[index_values[first_encounter], \"Time\"]\n",
    "            end_time = displacement_dataset.loc[index_values[-1], \"Time\"]\n",
    "            duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "            # Count sequences where dummy=1 (continuous stretches)\n",
    "            switches = np.diff(np.concatenate(([0], enc_dummy == 1.0, [0])))\n",
    "            enc_sequences = np.sum(switches == 1)\n",
    "\n",
    "            # Calculate total duration within encounter threshold\n",
    "            total_encounter_time = (\n",
    "                duration if enc_dummy[first_encounter:].all() else np.nan\n",
    "            )\n",
    "\n",
    "            return pd.DataFrame(\n",
    "                {\n",
    "                    \"enc_number\": [1],\n",
    "                    \"enc_start_time\": [start_time],\n",
    "                    \"enc_duration\": [duration],\n",
    "                    \"enc_sequences\": [enc_sequences],\n",
    "                    \"enc_sequences_duration\": [total_encounter_time],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            return pd.DataFrame(\n",
    "                {\n",
    "                    \"enc_number\": [0],\n",
    "                    \"enc_start_time\": [np.nan],\n",
    "                    \"enc_duration\": [0.0],\n",
    "                    \"enc_sequences\": [0],\n",
    "                    \"enc_sequences_duration\": [0.0],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Ensure first away index is at the start if needed\n",
    "    if away_indices[0] > 0:\n",
    "        away_indices = np.insert(away_indices, 0, 0)\n",
    "\n",
    "    # Make pairs of consecutive away indices\n",
    "    away_indices_pair = list(zip(away_indices, away_indices[1:]))\n",
    "\n",
    "    # Store output data\n",
    "    segments = []\n",
    "    segment_details = []\n",
    "    segment_start_times = []\n",
    "    segment_end_times = []\n",
    "    segment_durations = []\n",
    "    segment_encounter_counts = []\n",
    "    segment_encounter_durations = []\n",
    "\n",
    "    # Process each segment between consecutive away points\n",
    "    for start_idx, end_idx in away_indices_pair:\n",
    "        if end_idx - start_idx <= 1:  # Skip if segment is too small\n",
    "            continue\n",
    "\n",
    "        segment = enc_dummy[start_idx:end_idx]\n",
    "\n",
    "        if 1.0 not in segment:  # Skip if no encounters in segment\n",
    "            continue\n",
    "\n",
    "        segments.append(segment)\n",
    "        segment_details.append((start_idx, end_idx))\n",
    "\n",
    "        # Get segment start time (first encounter index)\n",
    "        first_encounter_offset = np.where(segment == 1.0)[0][0]\n",
    "        first_encounter_idx = start_idx + first_encounter_offset\n",
    "        segment_start_time = displacement_dataset.loc[\n",
    "            index_values[first_encounter_idx], \"Time\"\n",
    "        ]\n",
    "        segment_start_times.append(segment_start_time)\n",
    "\n",
    "        # Get segment end time (last index in segment)\n",
    "        segment_end_time = displacement_dataset.loc[index_values[end_idx - 1], \"Time\"]\n",
    "        segment_end_times.append(segment_end_time)\n",
    "\n",
    "        # Calculate segment duration\n",
    "        segment_duration = (segment_end_time - segment_start_time).total_seconds()\n",
    "        segment_durations.append(segment_duration)\n",
    "\n",
    "        # FIXED: Calculate encounter sub-sequences properly\n",
    "        # Find sub-sequences where ants are within encounter threshold (dummy=1)\n",
    "        segment_with_borders = np.concatenate(([0.5], segment, [0.5]))\n",
    "        transitions = np.diff(segment_with_borders)\n",
    "\n",
    "        # Specifically identify transitions TO and FROM value 1.0\n",
    "        transitions_to_one = np.where(transitions > 0)[\n",
    "            0\n",
    "        ]  # Identifies 0.51.0 transitions\n",
    "        transitions_from_one = np.where(transitions < 0)[\n",
    "            0\n",
    "        ]  # Identifies 1.00.5 transitions\n",
    "\n",
    "        # If the first element is already 1.0, insert a starting transition\n",
    "        if segment[0] == 1.0:\n",
    "            transitions_to_one = np.insert(transitions_to_one, 0, 0)\n",
    "\n",
    "        # If the last element is 1.0, add a final transition\n",
    "        if segment[-1] == 1.0:\n",
    "            transitions_from_one = np.append(transitions_from_one, len(segment))\n",
    "\n",
    "        # Ensure we have matching transition pairs by taking the minimum length\n",
    "        subsequence_count = min(len(transitions_to_one), len(transitions_from_one))\n",
    "        total_subsequence_duration = 0.0\n",
    "\n",
    "        for j in range(subsequence_count):\n",
    "            # Account for concatenation offset in to_one transitions\n",
    "            subseq_start_idx = start_idx + transitions_to_one[j]\n",
    "\n",
    "            # Account for concatenation and inclusive end in from_one transitions\n",
    "            subseq_end_idx = (\n",
    "                start_idx + transitions_from_one[j] - 1\n",
    "            )  # -1 for inclusive end\n",
    "\n",
    "            if subseq_end_idx >= subseq_start_idx:\n",
    "                subseq_start_time = displacement_dataset.loc[\n",
    "                    index_values[subseq_start_idx], \"Time\"\n",
    "                ]\n",
    "                subseq_end_time = displacement_dataset.loc[\n",
    "                    index_values[subseq_end_idx], \"Time\"\n",
    "                ]\n",
    "                subseq_duration = (subseq_end_time - subseq_start_time).total_seconds()\n",
    "                total_subsequence_duration += subseq_duration\n",
    "\n",
    "        segment_encounter_counts.append(subsequence_count)\n",
    "        segment_encounter_durations.append(total_subsequence_duration)\n",
    "\n",
    "    # Make sure all arrays have the same length\n",
    "    num_segments = len(segments)\n",
    "\n",
    "    if num_segments == 0:\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"enc_number\": [0],\n",
    "                \"enc_start_time\": [np.nan],\n",
    "                \"enc_duration\": [0.0],\n",
    "                \"enc_sequences\": [0],\n",
    "                \"enc_sequences_duration\": [0.0],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Verify all arrays have the same length\n",
    "    arrays = [\n",
    "        segment_start_times,\n",
    "        segment_durations,\n",
    "        segment_encounter_counts,\n",
    "        segment_encounter_durations,\n",
    "    ]\n",
    "    if not all(len(arr) == num_segments for arr in arrays):\n",
    "        # Debug information\n",
    "        print(\n",
    "            f\"Array length mismatch: segments={num_segments}, \"\n",
    "            f\"start_times={len(segment_start_times)}, \"\n",
    "            f\"durations={len(segment_durations)}, \"\n",
    "            f\"encounter_counts={len(segment_encounter_counts)}, \"\n",
    "            f\"encounter_durations={len(segment_encounter_durations)}\"\n",
    "        )\n",
    "\n",
    "        # Ensure all arrays have the same length by truncating to shortest\n",
    "        min_length = min(len(arr) for arr in arrays)\n",
    "        segment_start_times = segment_start_times[:min_length]\n",
    "        segment_durations = segment_durations[:min_length]\n",
    "        segment_encounter_counts = segment_encounter_counts[:min_length]\n",
    "        segment_encounter_durations = segment_encounter_durations[:min_length]\n",
    "\n",
    "    # Create encounter numbers\n",
    "    enc_numbers = np.arange(1, len(segment_start_times) + 1)\n",
    "\n",
    "    # Create output dataframe\n",
    "    enc_df = pd.DataFrame(\n",
    "        {\n",
    "            \"enc_number\": enc_numbers,\n",
    "            \"enc_start_time\": segment_start_times,\n",
    "            \"enc_duration\": segment_durations,\n",
    "            \"enc_sequences\": segment_encounter_counts,\n",
    "            \"enc_sequences_duration\": segment_encounter_durations,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return enc_df\n",
    "    # # Create pairs of consecutive away indices\n",
    "    # # This identifies segments bounded by \"away\" states\n",
    "    # segments = []\n",
    "    # segment_start_times = []\n",
    "    # segment_end_times = []\n",
    "    # segment_encounter_counts = []\n",
    "    # segment_encounter_durations = []\n",
    "\n",
    "    # # Process each segment bounded by \"away\" periods\n",
    "    # for i in range(len(away_indices) - 1):\n",
    "    #     start_idx = away_indices[i] + 1\n",
    "    #     end_idx = away_indices[i+1]\n",
    "\n",
    "    #     # Skip if segment is too small\n",
    "    #     if end_idx - start_idx <= 1:\n",
    "    #         continue\n",
    "\n",
    "    #     # Get the segment of dummy values\n",
    "    #     segment = enc_dummy[start_idx:end_idx]\n",
    "\n",
    "    #     # Check if segment contains any encounters (dummy=1)\n",
    "    #     if 1.0 not in segment:\n",
    "    #         continue\n",
    "\n",
    "    #     segments.append((start_idx, end_idx))\n",
    "\n",
    "    #     # Find first encounter (dummy=1) within segment\n",
    "    #     first_encounter_offset = np.where(segment == 1.0)[0][0]\n",
    "    #     first_encounter_idx = start_idx + first_encounter_offset\n",
    "\n",
    "    #     # Get start and end times for the segment\n",
    "    #     start_time = displacement_dataset.loc[index_values[first_encounter_idx], \"Time\"]\n",
    "    #     end_time = displacement_dataset.loc[index_values[end_idx-1], \"Time\"]\n",
    "\n",
    "    #     segment_start_times.append(start_time)\n",
    "    #     segment_end_times.append(end_time)\n",
    "\n",
    "    #     # Calculate overall duration\n",
    "    #     duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "    #     # Find sub-sequences where ants are within encounter threshold (dummy=1)\n",
    "    #     # We need to specifically identify transitions TO and FROM value 1.0\n",
    "    #     segment_with_borders = np.concatenate(([0.5], segment, [0.5]))\n",
    "    #     transitions = np.diff(segment_with_borders)\n",
    "    #     transitions_to_one = np.where(transitions > 0)[0]   # Identifies 0.51.0\n",
    "    #     transitions_from_one = np.where(transitions < 0)[0] # Identifies 1.00.5\n",
    "\n",
    "    #     # Ensure we have matching transition pairs\n",
    "    #     subsequence_count = min(len(transitions_to_one), len(transitions_from_one))\n",
    "    #     total_subsequence_duration = 0.0\n",
    "    #     subseq_durations = []\n",
    "\n",
    "    #     for j in range(subsequence_count):\n",
    "    #         # Get indices for transitions TO encounter and FROM encounter\n",
    "    #         subseq_start_idx = start_idx + transitions_to_one[j]\n",
    "    #         subseq_end_idx = start_idx + transitions_from_one[j] - 1  # -1 for inclusive end\n",
    "\n",
    "    #         if subseq_end_idx >= subseq_start_idx:\n",
    "    #             subseq_start_time = displacement_dataset.loc[index_values[subseq_start_idx], \"Time\"]\n",
    "    #             subseq_end_time = displacement_dataset.loc[index_values[subseq_end_idx], \"Time\"]\n",
    "    #             subseq_duration = (subseq_end_time - subseq_start_time).total_seconds()\n",
    "    #             subseq_durations.append(subseq_duration)\n",
    "    #             total_subsequence_duration += subseq_duration\n",
    "\n",
    "    # # If no valid segments were found\n",
    "    # if not segments:\n",
    "    #     return pd.DataFrame({\n",
    "    #         \"enc_number\": [0], \"enc_start_time\": [np.nan], \"enc_duration\": [0.0],\n",
    "    #         \"enc_sequences\": [0], \"enc_sequences_duration\": [0.0]\n",
    "    #     })\n",
    "\n",
    "    # # Calculate segment durations\n",
    "    # segment_durations = [(end_time - start_time).total_seconds()\n",
    "    #                      for start_time, end_time in zip(segment_start_times, segment_end_times)]\n",
    "\n",
    "    # # Create encounter numbers\n",
    "    # enc_numbers = np.arange(1, len(segments) + 1)\n",
    "\n",
    "    # # Create output dataframe directly with all data\n",
    "    # enc_df = pd.DataFrame({\n",
    "    #     \"enc_number\": enc_numbers,\n",
    "    #     \"enc_start_time\": segment_start_times,\n",
    "    #     \"enc_duration\": segment_durations,\n",
    "    #     \"enc_sequences\": segment_encounter_counts,\n",
    "    #     \"enc_sequences_duration\": segment_encounter_durations\n",
    "    # })\n",
    "\n",
    "    # return enc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_encounters(\n",
    "    start_time, end_time, exp, focal_ID, exp_day, encounter_threshold, away_threshold\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to obtain trajectories for focal and caregiver antIDs, merge by time and calculate displacement of each caregiver ID from the focal ID at every second\n",
    "    :param start_time: Starting time to obtain trajectories from. Passed on to function trajectory_output\n",
    "    :param end_time: Ending time to obtain trajectories from. Passed on to function trajectory_output\n",
    "    :param exp: Location of myrmidon file\n",
    "    :param focal_ID: Injured AntID\n",
    "    :param exp_day: Day of the experiment. This is added to the dataframe for identification\n",
    "    :param encounter_threshold: Threshold displacement to use as encounter\n",
    "    :param away_threshold: Threshold displacement to use as the start/end of an encounter\n",
    "    :return: Returns a datafarme containing the Time (in bins of 1s based on function trajectory_output), the focal and caregiver ID, the space in which the focal and caregiver ants are present, and the displacement between them (calculated as np.nan if they are in different spaces. In a CSV output this will be converted to a blank entry).\n",
    "    \"\"\"\n",
    "    start = datetime.now()\n",
    "    # # Focal Ant matcher\n",
    "    # focal_matcher = fm.Matcher.AntID(focal_ID)\n",
    "    # # Caregiver individual matchers\n",
    "    # # others = [fm.Matcher.AntID(x) for x in other_IDs]\n",
    "    # # Create single matcher object by unpacking the list within an Or Matcher\n",
    "    # #others_matcher = fm.Matcher.Or(*others)\n",
    "    # # Focal Ant trajectory\n",
    "    # focal_traj = trajectory_output(start_time, end_time, exp, focal_matcher)\n",
    "    # All ant trajectories\n",
    "    other_traj = trajectory_output_all(start_time, end_time, exp)\n",
    "    # Focal ant trajectory\n",
    "    focal_traj = other_traj[other_traj[\"AntID\"] == focal_ID]\n",
    "    # Sort Time column for both dataframes\n",
    "    other_traj = other_traj.sort_values(\"Time\")\n",
    "    focal_traj = focal_traj.sort_values(\"Time\")\n",
    "\n",
    "    # If focal trajectory is an empty dataframe, create a dataframe with na values for encounter parameters\n",
    "    if focal_traj.empty:\n",
    "        full_traj = other_traj.rename(columns={\"Space\": \"Space_ant\"})\n",
    "        full_traj[\"focalID\"] = focal_ID\n",
    "        full_traj[\"Space_focal\"] = full_traj[\"disp\"] = (\n",
    "            np.nan\n",
    "        )  # Create columns with na values\n",
    "        full_traj = full_traj[\n",
    "            [\"Time\", \"focalID\", \"AntID\", \"disp\", \"Space_focal\", \"Space_ant\"]\n",
    "        ]\n",
    "        full_traj[\"exp_day\"] = exp_day\n",
    "        full_traj = full_traj[full_traj[\"focalID\"] != full_traj[\"AntID\"]].reset_index()\n",
    "        # Group data frame, create columns with na and output encounter dataframe\n",
    "        enc_df = (\n",
    "            full_traj.groupby([\"exp_day\", \"focalID\", \"AntID\"])\n",
    "            .apply(lambda x: pd.Series([np.nan] * 5))\n",
    "            .reset_index()\n",
    "            .rename(\n",
    "                columns={\n",
    "                    0: \"enc_number\",\n",
    "                    1: \"enc_start_time\",\n",
    "                    2: \"enc_duration\",\n",
    "                    3: \"enc_sequences\",\n",
    "                    4: \"enc_sequences_duration\",\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            f\"{'Focal ID trajectory is empty for list item '}{exp_day}{' .Returning dataframe with no displacement and encounters calculated'}\"\n",
    "        )\n",
    "        return enc_df\n",
    "    # If trajectory of all other individuals is an empty dataframe, create a dataframe with na values for encounter parameters\n",
    "    if other_traj.empty:\n",
    "        full_traj = focal_traj.rename(\n",
    "            columns={\"AntID\": \"focalID\", \"Space\": \"Space_focal\"}\n",
    "        )\n",
    "        full_traj[\"AntID\"] = full_traj[\"Space_ant\"] = full_traj[\"disp\"] = (\n",
    "            np.nan\n",
    "        )  # Create columns with na values\n",
    "        full_traj = full_traj[\n",
    "            [\"Time\", \"focalID\", \"AntID\", \"disp\", \"Space_focal\", \"Space_ant\"]\n",
    "        ]\n",
    "        full_traj[\"exp_day\"] = exp_day\n",
    "        # Group data frame, create columns with na and output encounter dataframe\n",
    "        enc_df = (\n",
    "            full_traj.groupby([\"exp_day\", \"focalID\", \"AntID\"])\n",
    "            .apply(lambda x: pd.Series([np.nan] * 5))\n",
    "            .reset_index()\n",
    "            .rename(\n",
    "                columns={\n",
    "                    0: \"enc_number\",\n",
    "                    1: \"enc_start_time\",\n",
    "                    2: \"enc_duration\",\n",
    "                    3: \"enc_sequences\",\n",
    "                    4: \"enc_sequences_duration\",\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            f\"{'Caregiver ID trajectories are empty for list item '}{exp_day}{' .Returning dataframe with no displacement and encounters calculated'}\"\n",
    "        )\n",
    "        return enc_df\n",
    "\n",
    "    # Merge focal and caregiver trajectories on Time column using merge_asof to match nearest time values\n",
    "    full_traj = pd.merge_asof(\n",
    "        other_traj,\n",
    "        focal_traj,\n",
    "        on=\"Time\",\n",
    "        suffixes=(\"_ant\", \"_focal\"),\n",
    "        direction=\"nearest\",\n",
    "        tolerance=pd.Timedelta(\"1s\"),\n",
    "    )\n",
    "    # Obtain X coordinate and Y coordinate difference between Focal and Caregivers, for each row\n",
    "    full_traj[\"X_diff\"] = full_traj[\"X_coor_focal\"] - full_traj[\"X_coor_ant\"]\n",
    "    full_traj[\"Y_diff\"] = full_traj[\"Y_coor_focal\"] - full_traj[\"Y_coor_ant\"]\n",
    "    # Obtain displacement\n",
    "    full_traj[\"disp\"] = np.linalg.norm(\n",
    "        full_traj[[\"X_diff\", \"Y_diff\"]].to_numpy(), axis=1\n",
    "    )\n",
    "    # Rename columns\n",
    "    full_traj = full_traj.rename(\n",
    "        columns={\"AntID_focal\": \"focalID\", \"AntID_ant\": \"AntID\"}\n",
    "    )\n",
    "    # Subset specific columns\n",
    "    full_traj = full_traj[\n",
    "        [\"Time\", \"focalID\", \"AntID\", \"disp\", \"Space_focal\", \"Space_ant\"]\n",
    "    ]\n",
    "    # Add experimental day\n",
    "    full_traj[\"exp_day\"] = exp_day\n",
    "    # Remove instances where the focal ant's displacement is calculated wrt itself.\n",
    "    full_traj = full_traj[full_traj[\"focalID\"] != full_traj[\"AntID\"]].reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    # Replace with arbitrary high value of displacemeent if focal ant and caregiver are in different spaces. Use notnull to filter out instances where focal or caregiver space is not known. The higgh value will ensure that this case is always considered as > away_threshold in count_encounters function\n",
    "    full_traj.loc[\n",
    "        (\n",
    "            (full_traj.Space_focal.notnull())\n",
    "            & (full_traj.Space_ant.notnull())\n",
    "            & (full_traj.Space_focal != full_traj.Space_ant)\n",
    "        ),\n",
    "        \"disp\",\n",
    "    ] = 100000\n",
    "    # Apply encounter_duration function over grouped dataframe, reset index and rename columns\n",
    "    enc_df = (\n",
    "        full_traj.groupby([\"exp_day\", \"focalID\", \"AntID\"])\n",
    "        .apply(\n",
    "            lambda x: encounter_duration_optimized(\n",
    "                x, encounter_threshold, away_threshold\n",
    "            )\n",
    "        )\n",
    "        .reset_index()\n",
    "        .drop(\"level_3\", axis=1)\n",
    "    )\n",
    "    end = datetime.now()\n",
    "    print(\n",
    "        f\"{'Encounters for experimental day '}{exp_day}{' calculated in '}{end - start}\"\n",
    "    )\n",
    "    return enc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_myrmidon = \"/media/ebiag/Ebi-2/Woundcare Experiment1/Cfell_wound_col42.myrmidon\"\n",
    "exp = fm.Experiment.Open(f_myrmidon)\n",
    "focal_ID = 106\n",
    "start_time = datetime(2022, 5, 2, 16, 3).astimezone(tz=None)\n",
    "end_time = start_time + timedelta(hours=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_traj = trajectory_output_all(start_time, end_time, exp)\n",
    "# Focal ant trajectory\n",
    "focal_traj = other_traj[other_traj[\"AntID\"] == focal_ID]\n",
    "# Sort Time column for both dataframes\n",
    "other_traj = other_traj.sort_values(\"Time\")\n",
    "focal_traj = focal_traj.sort_values(\"Time\")\n",
    "# Merge focal and caregiver trajectories on Time column using merge_asof to match nearest time values\n",
    "full_traj = pd.merge_asof(\n",
    "    other_traj,\n",
    "    focal_traj,\n",
    "    on=\"Time\",\n",
    "    suffixes=(\"_ant\", \"_focal\"),\n",
    "    direction=\"nearest\",\n",
    "    tolerance=pd.Timedelta(\"1s\"),\n",
    ")\n",
    "# Obtain X coordinate and Y coordinate difference between Focal and Caregivers, for each row\n",
    "full_traj[\"X_diff\"] = full_traj[\"X_coor_focal\"] - full_traj[\"X_coor_ant\"]\n",
    "full_traj[\"Y_diff\"] = full_traj[\"Y_coor_focal\"] - full_traj[\"Y_coor_ant\"]\n",
    "# Obtain displacement\n",
    "full_traj[\"disp\"] = np.linalg.norm(full_traj[[\"X_diff\", \"Y_diff\"]].to_numpy(), axis=1)\n",
    "# Rename columns\n",
    "full_traj = full_traj.rename(columns={\"AntID_focal\": \"focalID\", \"AntID_ant\": \"AntID\"})\n",
    "# Subset specific columns\n",
    "full_traj = full_traj[[\"Time\", \"focalID\", \"AntID\", \"disp\", \"Space_focal\", \"Space_ant\"]]\n",
    "# Add experimental day\n",
    "full_traj[\"exp_day\"] = 1\n",
    "# Remove instances where the focal ant's displacement is calculated wrt itself.\n",
    "full_traj = full_traj[full_traj[\"focalID\"] != full_traj[\"AntID\"]].reset_index(drop=True)\n",
    "# Replace with arbitrary high value of displacemeent if focal ant and caregiver are in different spaces. Use notnull to filter out instances where focal or caregiver space is not known. The higgh value will ensure that this case is always considered as > away_threshold in count_encounters function\n",
    "full_traj.loc[\n",
    "    (\n",
    "        (full_traj.Space_focal.notnull())\n",
    "        & (full_traj.Space_ant.notnull())\n",
    "        & (full_traj.Space_focal != full_traj.Space_ant)\n",
    "    ),\n",
    "    \"disp\",\n",
    "] = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply encounter_duration function over grouped dataframe, reset index and rename columns\n",
    "enc_df = (\n",
    "    full_traj.groupby([\"exp_day\", \"focalID\", \"AntID\"])\n",
    "    .apply(lambda x: encounter_duration(x, encounter_threshold, away_threshold))\n",
    "    .reset_index()\n",
    "    .drop(\"level_3\", axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply encounter_duration function over grouped dataframe, reset index and rename columns\n",
    "enc_df_optimised = (\n",
    "    full_traj.groupby([\"exp_day\", \"focalID\", \"AntID\"])\n",
    "    .apply(\n",
    "        lambda x: encounter_duration_optimized(x, encounter_threshold, away_threshold)\n",
    "    )\n",
    "    .reset_index()\n",
    "    .drop(\"level_3\", axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare enc_df and enc_df optimised to see whether they match up\n",
    "enc_df_merged = enc_df.merge(\n",
    "    enc_df_optimised,\n",
    "    on=[\"exp_day\", \"focalID\", \"AntID\"],\n",
    "    suffixes=(\"_original\", \"_optimized\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test\n",
    "ant_id = 1  # Use a specific ant ID that shows differences\n",
    "focal_id = 106  # Use your focal ant ID\n",
    "\n",
    "# Get data for just this pair\n",
    "pair_data = full_traj[\n",
    "    (full_traj[\"AntID\"] == ant_id) & (full_traj[\"focalID\"] == focal_id)\n",
    "].copy()\n",
    "\n",
    "# Run both functions\n",
    "orig_result = encounter_duration(pair_data, encounter_threshold, away_threshold)\n",
    "optimised_result = encounter_duration_optimized(\n",
    "    pair_data, encounter_threshold, away_threshold\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacement_dataset = full_traj[\n",
    "    (full_traj[\"AntID\"] == ant_id) & (full_traj[\"focalID\"] == focal_id)\n",
    "].copy()\n",
    "# Work with copies of only the columns we need\n",
    "disp_values = displacement_dataset[\"disp\"].values\n",
    "# time_values = displacement_dataset[\"Time\"].values\n",
    "index_values = displacement_dataset.index.values\n",
    "# Interpolate missing values only if there are NaNs\n",
    "if np.isnan(disp_values).any():\n",
    "    print(\"NA values\")\n",
    "    valid_indices = ~np.isnan(disp_values)\n",
    "    if np.any(valid_indices):  # Make sure there's at least one valid value\n",
    "        valid_idx = np.where(valid_indices)[0]\n",
    "        valid_disp = disp_values[valid_indices]\n",
    "        # Create interpolator function\n",
    "        interp_indices = np.arange(len(disp_values))\n",
    "        disp_values = np.interp(interp_indices, valid_idx, valid_disp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encounter dummy values using NumPy's faster conditional selection\n",
    "# 1.0: within encounter threshold, 0.5: between encounter and away, 0.0: beyond away threshold\n",
    "enc_dummy = np.select(\n",
    "    [\n",
    "        disp_values <= encounter_threshold,\n",
    "        (disp_values > encounter_threshold) & (disp_values <= away_threshold),\n",
    "    ],\n",
    "    [1.0, 0.5],\n",
    "    default=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices where ants are beyond away threshold (dummy=0)\n",
    "away_indices = np.where(enc_dummy == 0.0)[0]\n",
    "if away_indices[0] > 0:\n",
    "    away_indices = np.insert(away_indices, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make pairs of consecutive away indices\n",
    "away_indices_pair = list(zip(away_indices, away_indices[1:]))\n",
    "\n",
    "# Store output data\n",
    "segments = []\n",
    "segment_details = []\n",
    "segment_start_times = []\n",
    "segment_end_times = []\n",
    "segment_durations = []\n",
    "segment_encounter_counts = []\n",
    "segment_encounter_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment with borders: [0.5 0.  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1.  1.\n",
      " 1.  1.  1.  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "Differences: [-0.5  0.5  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.5  0.   0.   0.   0.  -0.5  0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0. ]\n",
      "Change indices: [ 0  1 33 38]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 79994 is out of bounds for axis 0 with size 79964",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m subseq_start_idx \u001b[38;5;241m=\u001b[39m start_indices[j]\n\u001b[1;32m     68\u001b[0m subseq_end_idx \u001b[38;5;241m=\u001b[39m end_indices[j]\n\u001b[0;32m---> 70\u001b[0m subseq_start_time \u001b[38;5;241m=\u001b[39m displacement_dataset\u001b[38;5;241m.\u001b[39mloc[\u001b[43mindex_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43msubseq_start_idx\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     71\u001b[0m subseq_end_time \u001b[38;5;241m=\u001b[39m displacement_dataset\u001b[38;5;241m.\u001b[39mloc[index_values[subseq_end_idx], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     72\u001b[0m subseq_duration \u001b[38;5;241m=\u001b[39m (subseq_end_time \u001b[38;5;241m-\u001b[39m subseq_start_time)\u001b[38;5;241m.\u001b[39mtotal_seconds()\n",
      "\u001b[0;31mIndexError\u001b[0m: index 79994 is out of bounds for axis 0 with size 79964"
     ]
    }
   ],
   "source": [
    "# Process each segment between consecutive away points\n",
    "for start_idx, end_idx in away_indices_pair:\n",
    "    if end_idx - start_idx <= 1:  # Skip if segment is too small\n",
    "        continue\n",
    "    segment = enc_dummy[start_idx:end_idx]\n",
    "\n",
    "    if 1.0 not in segment:  # Skip if no encounters in segment\n",
    "        continue\n",
    "\n",
    "    segments.append(segment)\n",
    "    segment_details.append((start_idx, end_idx))\n",
    "\n",
    "    # Get segment start time (first encounter index)\n",
    "    first_encounter_offset = np.where(segment == 1.0)[0][0]\n",
    "    first_encounter_idx = start_idx + first_encounter_offset\n",
    "    segment_start_time = displacement_dataset.loc[\n",
    "        index_values[first_encounter_idx], \"Time\"\n",
    "    ]\n",
    "    segment_start_times.append(segment_start_time)\n",
    "\n",
    "    # Get segment end time (last index in segment)\n",
    "    segment_end_time = displacement_dataset.loc[index_values[end_idx - 1], \"Time\"]\n",
    "    segment_end_times.append(segment_end_time)\n",
    "\n",
    "    # Calculate segment duration\n",
    "    segment_duration = (segment_end_time - segment_start_time).total_seconds()\n",
    "    segment_durations.append(segment_duration)\n",
    "\n",
    "    # FIXED: Calculate encounter sub-sequences properly\n",
    "# Directly mimic the original time_within_encounter_threshold function\n",
    "# Add 0.5 to beginning and end for boundary detection\n",
    "segment_with_borders = np.concatenate(([0.5], segment, [0.5]))\n",
    "differences = np.diff(segment_with_borders)\n",
    "change_indices = np.where(differences != 0)[0]\n",
    "\n",
    "# Debug output\n",
    "print(f\"Segment with borders: {segment_with_borders}\")\n",
    "print(f\"Differences: {differences}\")\n",
    "print(f\"Change indices: {change_indices}\")\n",
    "\n",
    "# The original function pairs indices differently\n",
    "# It groups them by even/odd positions\n",
    "start_indices = []\n",
    "end_indices = []\n",
    "\n",
    "# Only proceed if we have enough change points\n",
    "if len(change_indices) >= 2:\n",
    "    # Get every even-indexed change point (0, 2, 4...)\n",
    "    for i in range(0, len(change_indices), 2):\n",
    "        if i + 1 < len(change_indices):\n",
    "            # Convert from change index to segment index (adjusting for the added 0.5 at start)\n",
    "            start_idx_in_segment = change_indices[i] - 1 + start_idx\n",
    "            # The original code does \"-1\" to get inclusive end\n",
    "            end_idx_in_segment = change_indices[i + 1] - 1 - 1 + start_idx\n",
    "\n",
    "            # Only keep pairs where startend and the segment contains value 1.0\n",
    "            if end_idx_in_segment >= start_idx_in_segment:\n",
    "                # Check if this range includes encounter values\n",
    "                segment_slice = segment[\n",
    "                    change_indices[i] - 1 : change_indices[i + 1] - 1\n",
    "                ]\n",
    "                if 1.0 in segment_slice:\n",
    "                    start_indices.append(start_idx_in_segment)\n",
    "                    end_indices.append(end_idx_in_segment)\n",
    "\n",
    "subsequence_count = len(start_indices)\n",
    "total_subsequence_duration = 0.0\n",
    "\n",
    "# Calculate duration for each subsequence\n",
    "for j in range(subsequence_count):\n",
    "    subseq_start_idx = start_indices[j]\n",
    "    subseq_end_idx = end_indices[j]\n",
    "\n",
    "    subseq_start_time = displacement_dataset.loc[index_values[subseq_start_idx], \"Time\"]\n",
    "    subseq_end_time = displacement_dataset.loc[index_values[subseq_end_idx], \"Time\"]\n",
    "    subseq_duration = (subseq_end_time - subseq_start_time).total_seconds()\n",
    "    total_subsequence_duration += subseq_duration\n",
    "\n",
    "segment_encounter_counts.append(subsequence_count)\n",
    "segment_encounter_durations.append(total_subsequence_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of segments: 42\n",
      "Segment start times: [Timestamp('2022-05-02 16:17:17.602161'), Timestamp('2022-05-02 17:44:48.659337'), Timestamp('2022-05-02 18:50:05.672149'), Timestamp('2022-05-02 20:00:49.707241'), Timestamp('2022-05-02 21:26:30.163532'), Timestamp('2022-05-02 21:32:26.967445'), Timestamp('2022-05-02 21:57:31.583962'), Timestamp('2022-05-02 16:17:17.602161'), Timestamp('2022-05-02 17:44:48.659337'), Timestamp('2022-05-02 18:50:05.672149'), Timestamp('2022-05-02 20:00:49.707241'), Timestamp('2022-05-02 21:26:30.163532'), Timestamp('2022-05-02 21:32:26.967445'), Timestamp('2022-05-02 21:57:31.583962'), Timestamp('2022-05-02 16:17:17.602161'), Timestamp('2022-05-02 17:44:48.659337'), Timestamp('2022-05-02 18:50:05.672149'), Timestamp('2022-05-02 20:00:49.707241'), Timestamp('2022-05-02 21:26:30.163532'), Timestamp('2022-05-02 21:32:26.967445'), Timestamp('2022-05-02 21:57:31.583962'), Timestamp('2022-05-02 16:17:17.602161'), Timestamp('2022-05-02 17:44:48.659337'), Timestamp('2022-05-02 18:50:05.672149'), Timestamp('2022-05-02 20:00:49.707241'), Timestamp('2022-05-02 21:26:30.163532'), Timestamp('2022-05-02 21:32:26.967445'), Timestamp('2022-05-02 21:57:31.583962'), Timestamp('2022-05-02 16:17:17.602161'), Timestamp('2022-05-02 17:44:48.659337'), Timestamp('2022-05-02 18:50:05.672149'), Timestamp('2022-05-02 20:00:49.707241'), Timestamp('2022-05-02 21:26:30.163532'), Timestamp('2022-05-02 21:32:26.967445'), Timestamp('2022-05-02 21:57:31.583962'), Timestamp('2022-05-02 16:17:17.602161'), Timestamp('2022-05-02 17:44:48.659337'), Timestamp('2022-05-02 18:50:05.672149'), Timestamp('2022-05-02 20:00:49.707241'), Timestamp('2022-05-02 21:26:30.163532'), Timestamp('2022-05-02 21:32:26.967445'), Timestamp('2022-05-02 21:57:31.583962')]\n",
      "Segment end times: [Timestamp('2022-05-02 16:17:31.802316'), Timestamp('2022-05-02 17:45:02.259485'), Timestamp('2022-05-02 18:50:10.872205'), Timestamp('2022-05-02 20:03:44.709154'), Timestamp('2022-05-02 21:26:44.163685'), Timestamp('2022-05-02 21:32:28.567464'), Timestamp('2022-05-02 21:58:27.384576'), Timestamp('2022-05-02 16:17:31.802316'), Timestamp('2022-05-02 17:45:02.259485'), Timestamp('2022-05-02 18:50:10.872205'), Timestamp('2022-05-02 20:03:44.709154'), Timestamp('2022-05-02 21:26:44.163685'), Timestamp('2022-05-02 21:32:28.567464'), Timestamp('2022-05-02 21:58:27.384576'), Timestamp('2022-05-02 16:17:31.802316'), Timestamp('2022-05-02 17:45:02.259485'), Timestamp('2022-05-02 18:50:10.872205'), Timestamp('2022-05-02 20:03:44.709154'), Timestamp('2022-05-02 21:26:44.163685'), Timestamp('2022-05-02 21:32:28.567464'), Timestamp('2022-05-02 21:58:27.384576'), Timestamp('2022-05-02 16:17:31.802316'), Timestamp('2022-05-02 17:45:02.259485'), Timestamp('2022-05-02 18:50:10.872205'), Timestamp('2022-05-02 20:03:44.709154'), Timestamp('2022-05-02 21:26:44.163685'), Timestamp('2022-05-02 21:32:28.567464'), Timestamp('2022-05-02 21:58:27.384576'), Timestamp('2022-05-02 16:17:31.802316'), Timestamp('2022-05-02 17:45:02.259485'), Timestamp('2022-05-02 18:50:10.872205'), Timestamp('2022-05-02 20:03:44.709154'), Timestamp('2022-05-02 21:26:44.163685'), Timestamp('2022-05-02 21:32:28.567464'), Timestamp('2022-05-02 21:58:27.384576'), Timestamp('2022-05-02 16:17:31.802316'), Timestamp('2022-05-02 17:45:02.259485'), Timestamp('2022-05-02 18:50:10.872205'), Timestamp('2022-05-02 20:03:44.709154'), Timestamp('2022-05-02 21:26:44.163685'), Timestamp('2022-05-02 21:32:28.567464'), Timestamp('2022-05-02 21:58:27.384576')]\n",
      "Segment durations: [14.200155, 13.600148, 5.200056, 175.001913, 14.000153, 1.600019, 55.800614, 14.200155, 13.600148, 5.200056, 175.001913, 14.000153, 1.600019, 55.800614, 14.200155, 13.600148, 5.200056, 175.001913, 14.000153, 1.600019, 55.800614, 14.200155, 13.600148, 5.200056, 175.001913, 14.000153, 1.600019, 55.800614, 14.200155, 13.600148, 5.200056, 175.001913, 14.000153, 1.600019, 55.800614, 14.200155, 13.600148, 5.200056, 175.001913, 14.000153, 1.600019, 55.800614]\n",
      "Segment encounter counts: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1]\n",
      "Segment encounter durations: [0.0, 6.400069, 0.0, 4.400048, 6.400068, 0.0, 0.800009, 0.0, 6.400069, 0.0, 4.400048, 6.400068, 0.0, 0.800009, 0.0, 6.400069, 0.0, 4.400048, 6.400068, 0.0, 0.800009, 0.0, 6.400069, 0.0, 4.400048, 6.400068, 0.0, 0.800009, 0.0, 6.400069, 0.0, 4.400048, 6.400068, 0.0, 0.800009, 1.600017, 6.400069, 0.0, 4.400048, 6.400068, 0.200002, 0.800009]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of segments: {len(segments)}\")\n",
    "print(f\"Segment start times: {segment_start_times}\")\n",
    "print(f\"Segment end times: {segment_end_times}\")\n",
    "print(f\"Segment durations: {segment_durations}\")\n",
    "print(f\"Segment encounter counts: {segment_encounter_counts}\")\n",
    "print(f\"Segment encounter durations: {segment_encounter_durations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== STEP-BY-STEP COMPARISON =====\n",
      "\n",
      "Input dataset shape: (79964, 8)\n",
      "First few rows of input data:\n",
      "                            Time  focalID  AntID      disp  Space_focal  \\\n",
      "86025 2022-05-02 16:07:06.945717    106.0      1  100000.0          2.0   \n",
      "86117 2022-05-02 16:07:07.145721    106.0      1  100000.0          2.0   \n",
      "86188 2022-05-02 16:07:07.345724    106.0      1  100000.0          2.0   \n",
      "86237 2022-05-02 16:07:07.545728    106.0      1  100000.0          2.0   \n",
      "86322 2022-05-02 16:07:07.745732    106.0      1  100000.0          2.0   \n",
      "\n",
      "       Space_ant  exp_day enc_dummy  \n",
      "86025          1        1       0.0  \n",
      "86117          1        1       0.0  \n",
      "86188          1        1       0.0  \n",
      "86237          1        1       0.0  \n",
      "86322          1        1       0.0  \n",
      "\n",
      "STEP 1: Check for minimum dataset size\n",
      "Dataset has 79964 rows, proceeding with analysis\n",
      "\n",
      "STEP 2: Interpolate missing displacement values\n",
      "Original: Using pandas interpolate with forward fill\n",
      "Original: NaN values before interpolation: 0\n",
      "Original: NaN values after interpolation: 0\n",
      "\n",
      "Optimized: Using NumPy interpolation\n",
      "Optimized: NaN values before interpolation: 0\n",
      "Optimized: NaN values after interpolation: 0\n",
      "\n",
      "Comparison of interpolated values (first 5):\n",
      "Original: [100000. 100000. 100000. 100000. 100000.]\n",
      "Optimized: [100000. 100000. 100000. 100000. 100000.]\n",
      " Interpolated values match\n",
      "\n",
      "STEP 3: Create encounter dummy values\n",
      "Original: Using pd.cut\n",
      "Optimized: Using np.select\n",
      "\n",
      "Dummy value counts:\n",
      "Original: {0.0: 75748, 0.5: 4130, 1.0: 86}\n",
      "Optimized: {0.0: 75748, 0.5: 4130, 1.0: 86}\n",
      "\n",
      "First 5 dummy values:\n",
      "Original: [0. 0. 0. 0. 0.]\n",
      "Optimized: [0. 0. 0. 0. 0.]\n",
      " Dummy values match\n",
      "\n",
      "STEP 4: Check if encounters exist\n",
      "Original: Has encounters: True\n",
      "Optimized: Has encounters: True\n",
      " Encounter existence check matches\n",
      "\n",
      "STEP 5: Find indices where ants are beyond away threshold (dummy=0)\n",
      "Original: Found 75748 'away' indices\n",
      "Original: First few away indices: [86025 86117 86188 86237 86322]\n",
      "Optimized: Found 75748 'away' indices\n",
      "Optimized: First few away indices: [0 1 2 3 4]\n",
      "Optimized (mapped to original indices): [86025 86117 86188 86237 86322]\n",
      " Number of 'away' indices matches\n",
      "\n",
      "STEP 6: Insert starting index if needed\n",
      "Original: First index in dataset is 86025\n",
      "Original: No need to insert starting index\n",
      "Optimized: No need to insert starting index\n",
      "Original: Away indices after insertion: [86025 86117 86188 86237 86322]\n",
      "Optimized: Away indices after insertion: [0 1 2 3 4]\n",
      " Number of 'away' indices after insertion matches\n"
     ]
    }
   ],
   "source": [
    "# Sample data - replace this with your actual data for the focal 106, ant ID 1 case\n",
    "# Let's assume pair_data is already defined with your real data\n",
    "\n",
    "print(\"===== STEP-BY-STEP COMPARISON =====\\n\")\n",
    "\n",
    "# Make copies to avoid modifying the original data\n",
    "original_data = pair_data.copy()\n",
    "optimized_data = pair_data.copy()\n",
    "\n",
    "print(f\"Input dataset shape: {pair_data.shape}\")\n",
    "print(f\"First few rows of input data:\\n{pair_data.head()}\\n\")\n",
    "\n",
    "# STEP 1: Check if dataset is too small\n",
    "print(\"STEP 1: Check for minimum dataset size\")\n",
    "if len(pair_data) < 3:\n",
    "    print(\"Dataset too small, both functions will return empty result\")\n",
    "else:\n",
    "    print(f\"Dataset has {len(pair_data)} rows, proceeding with analysis\")\n",
    "\n",
    "# STEP 2: Interpolate missing values\n",
    "print(\"\\nSTEP 2: Interpolate missing displacement values\")\n",
    "\n",
    "# Original version\n",
    "print(\"Original: Using pandas interpolate with forward fill\")\n",
    "nan_before_orig = original_data[\"disp\"].isna().sum()\n",
    "print(f\"Original: NaN values before interpolation: {nan_before_orig}\")\n",
    "\n",
    "original_data[\"disp\"].interpolate(\n",
    "    method=\"linear\", limit_direction=\"forward\", inplace=True\n",
    ")\n",
    "\n",
    "nan_after_orig = original_data[\"disp\"].isna().sum()\n",
    "print(f\"Original: NaN values after interpolation: {nan_after_orig}\")\n",
    "\n",
    "# Optimized version\n",
    "print(\"\\nOptimized: Using NumPy interpolation\")\n",
    "disp_values = optimized_data[\"disp\"].values\n",
    "index_values = optimized_data.index.values\n",
    "\n",
    "nan_before_opt = np.isnan(disp_values).sum()\n",
    "print(f\"Optimized: NaN values before interpolation: {nan_before_opt}\")\n",
    "\n",
    "if np.isnan(disp_values).any():\n",
    "    valid_indices = ~np.isnan(disp_values)\n",
    "    if np.any(valid_indices):\n",
    "        valid_idx = np.where(valid_indices)[0]\n",
    "        valid_disp = disp_values[valid_indices]\n",
    "        interp_indices = np.arange(len(disp_values))\n",
    "        disp_values = np.interp(interp_indices, valid_idx, valid_disp)\n",
    "\n",
    "nan_after_opt = np.isnan(disp_values).sum()\n",
    "print(f\"Optimized: NaN values after interpolation: {nan_after_opt}\")\n",
    "\n",
    "# Compare interpolated values\n",
    "print(\"\\nComparison of interpolated values (first 5):\")\n",
    "print(f\"Original: {original_data['disp'].head().values}\")\n",
    "print(f\"Optimized: {disp_values[:5]}\")\n",
    "\n",
    "if not np.allclose(\n",
    "    original_data[\"disp\"].head().values, disp_values[:5], equal_nan=True\n",
    "):\n",
    "    print(\" DIFFERENCE DETECTED: Interpolated values don't match\")\n",
    "else:\n",
    "    print(\" Interpolated values match\")\n",
    "\n",
    "# STEP 3: Create encounter dummy column\n",
    "print(\"\\nSTEP 3: Create encounter dummy values\")\n",
    "\n",
    "# Original version\n",
    "print(\"Original: Using pd.cut\")\n",
    "original_data.loc[:, [\"enc_dummy\"]] = pd.cut(\n",
    "    original_data.disp,\n",
    "    [0, encounter_threshold, away_threshold, np.inf],\n",
    "    labels=[1, 0.5, 0],\n",
    ")\n",
    "original_data = original_data.astype({\"enc_dummy\": float})\n",
    "\n",
    "# Optimized version\n",
    "print(\"Optimized: Using np.select\")\n",
    "enc_dummy = np.select(\n",
    "    [\n",
    "        disp_values <= encounter_threshold,\n",
    "        (disp_values > encounter_threshold) & (disp_values <= away_threshold),\n",
    "    ],\n",
    "    [1.0, 0.5],\n",
    "    default=0.0,\n",
    ")\n",
    "\n",
    "# Compare dummy value counts\n",
    "print(\"\\nDummy value counts:\")\n",
    "orig_counts = original_data[\"enc_dummy\"].value_counts().to_dict()\n",
    "opt_counts = pd.Series(enc_dummy).value_counts().to_dict()\n",
    "print(f\"Original: {orig_counts}\")\n",
    "print(f\"Optimized: {opt_counts}\")\n",
    "\n",
    "# Compare the first 5 values\n",
    "print(\"\\nFirst 5 dummy values:\")\n",
    "print(f\"Original: {original_data['enc_dummy'].head().values}\")\n",
    "print(f\"Optimized: {enc_dummy[:5]}\")\n",
    "\n",
    "if not np.allclose(original_data[\"enc_dummy\"].head().values, enc_dummy[:5]):\n",
    "    print(\" DIFFERENCE DETECTED: Dummy values don't match\")\n",
    "else:\n",
    "    print(\" Dummy values match\")\n",
    "\n",
    "# STEP 4: Check if encounters exist\n",
    "print(\"\\nSTEP 4: Check if encounters exist\")\n",
    "\n",
    "# Original version\n",
    "has_encounters_orig = 1 in original_data.enc_dummy.values\n",
    "print(f\"Original: Has encounters: {has_encounters_orig}\")\n",
    "\n",
    "# Optimized version\n",
    "has_encounters_opt = 1.0 in enc_dummy\n",
    "print(f\"Optimized: Has encounters: {has_encounters_opt}\")\n",
    "\n",
    "if has_encounters_orig != has_encounters_opt:\n",
    "    print(\" DIFFERENCE DETECTED: Encounter existence check doesn't match\")\n",
    "else:\n",
    "    print(\" Encounter existence check matches\")\n",
    "\n",
    "if not has_encounters_orig:\n",
    "    print(\"No encounters detected, both functions will return empty result\")\n",
    "    # Exit here\n",
    "\n",
    "# STEP 5: Find 'away' indices\n",
    "print(\"\\nSTEP 5: Find indices where ants are beyond away threshold (dummy=0)\")\n",
    "\n",
    "# Original version\n",
    "away_indices_orig = original_data[original_data[\"enc_dummy\"] == 0].index.values\n",
    "print(f\"Original: Found {len(away_indices_orig)} 'away' indices\")\n",
    "if len(away_indices_orig) > 0:\n",
    "    print(\n",
    "        f\"Original: First few away indices: {away_indices_orig[:5] if len(away_indices_orig) >= 5 else away_indices_orig}\"\n",
    "    )\n",
    "\n",
    "# Optimized version\n",
    "away_indices_opt = np.where(enc_dummy == 0.0)[0]\n",
    "print(f\"Optimized: Found {len(away_indices_opt)} 'away' indices\")\n",
    "if len(away_indices_opt) > 0:\n",
    "    print(\n",
    "        f\"Optimized: First few away indices: {away_indices_opt[:5] if len(away_indices_opt) >= 5 else away_indices_opt}\"\n",
    "    )\n",
    "\n",
    "    # Map optimized indices to original indices for comparison\n",
    "    mapped_indices = index_values[away_indices_opt]\n",
    "    print(\n",
    "        f\"Optimized (mapped to original indices): {mapped_indices[:5] if len(mapped_indices) >= 5 else mapped_indices}\"\n",
    "    )\n",
    "\n",
    "if len(away_indices_orig) != len(away_indices_opt):\n",
    "    print(\n",
    "        f\" DIFFERENCE DETECTED: Number of 'away' indices doesn't match ({len(away_indices_orig)} vs {len(away_indices_opt)})\"\n",
    "    )\n",
    "else:\n",
    "    print(\" Number of 'away' indices matches\")\n",
    "\n",
    "# Step 6: Insert starting index if needed\n",
    "print(\"\\nSTEP 6: Insert starting index if needed\")\n",
    "\n",
    "# Original version\n",
    "first_index_orig = np.take(original_data.index.values, 0)\n",
    "print(f\"Original: First index in dataset is {first_index_orig}\")\n",
    "\n",
    "if away_indices_orig.size > 0 and away_indices_orig[0] != first_index_orig:\n",
    "    print(f\"Original: Inserting starting index {first_index_orig}\")\n",
    "    away_indices_orig = np.insert(away_indices_orig, 0, first_index_orig)\n",
    "else:\n",
    "    print(\"Original: No need to insert starting index\")\n",
    "\n",
    "# Optimized version\n",
    "if len(away_indices_opt) > 0 and away_indices_opt[0] > 0:\n",
    "    print(f\"Optimized: Inserting starting index 0\")\n",
    "    away_indices_opt = np.insert(away_indices_opt, 0, 0)\n",
    "else:\n",
    "    print(\"Optimized: No need to insert starting index\")\n",
    "\n",
    "print(\n",
    "    f\"Original: Away indices after insertion: {away_indices_orig[:5] if len(away_indices_orig) >= 5 else away_indices_orig}\"\n",
    ")\n",
    "print(\n",
    "    f\"Optimized: Away indices after insertion: {away_indices_opt[:5] if len(away_indices_opt) >= 5 else away_indices_opt}\"\n",
    ")\n",
    "\n",
    "# Count after insertion\n",
    "if len(away_indices_orig) != len(away_indices_opt):\n",
    "    print(\n",
    "        f\" DIFFERENCE DETECTED: Number of 'away' indices after insertion doesn't match ({len(away_indices_orig)} vs {len(away_indices_opt)})\"\n",
    "    )\n",
    "else:\n",
    "    print(\" Number of 'away' indices after insertion matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 7: Create pairs of consecutive away indices\n",
      "Original: Number of away index pairs: 75747\n",
      "Original: First few pairs: [(86025, 86117), (86117, 86188), (86188, 86237)]\n",
      "Optimized: Number of away index pairs: 75747\n",
      "Optimized: First few pairs: [(0, 1), (1, 2), (2, 3)]\n",
      " Number of away index pairs matches\n",
      "\n",
      "STEP 8: Extract sequences between away points\n",
      "Original: Extracted 75747 sequences between 'away' points\n",
      "Original: After removing sequences with size  1: 43\n",
      "Original: After keeping only sequences with encounters: 7\n",
      "Original: First sequence details:\n",
      "  Sequence 1: Length=63, Values=[1.  1.  1.  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 1.  1.  1.  1.  1.  1.  1.  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "  Index values: [297324 297394 297461 297600 297738 297808 297877 297946 298012 298075\n",
      " 298143 298212 298283 298351 298418 298484 298550 298616 298681 298749\n",
      " 298817 298888 298958 299025 299091 299160 299232 299299 299363 299428\n",
      " 299491 299556 299623 300075 300138 300267 300330 300398 300462 300525\n",
      " 300588 300654 300721 300787 300854 300919 300985 301052 301119 301185\n",
      " 301252 301316 301382 301448 301512 301577 301640 301704 301771 301839\n",
      " 301908 301977 302044]\n",
      "  Contains 10 values of 1 (encounters)\n",
      "Optimized: Found 7 sequences with encounters\n",
      "Optimized: First sequence details:\n",
      "  Sequence 1: Length=64, Start=2013, End=2077\n",
      "  Values: [0.  1.  1.  1.  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 1.  1.  1.  1.  1.  1.  1.  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "  Contains 10 values of 1 (encounters)\n",
      " Number of encounter sequences matches\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Create pairs of consecutive away indices\n",
    "print(\"\\nSTEP 7: Create pairs of consecutive away indices\")\n",
    "\n",
    "# Original version\n",
    "away_indices_pair_orig = list(zip(away_indices_orig, away_indices_orig[1:]))\n",
    "print(f\"Original: Number of away index pairs: {len(away_indices_pair_orig)}\")\n",
    "if len(away_indices_pair_orig) > 0:\n",
    "    print(f\"Original: First few pairs: {away_indices_pair_orig[:3]}\")\n",
    "\n",
    "# Optimized version\n",
    "away_indices_pair_opt = list(zip(away_indices_opt, away_indices_opt[1:]))\n",
    "print(f\"Optimized: Number of away index pairs: {len(away_indices_pair_opt)}\")\n",
    "if len(away_indices_pair_opt) > 0:\n",
    "    print(f\"Optimized: First few pairs: {away_indices_pair_opt[:3]}\")\n",
    "\n",
    "if len(away_indices_pair_orig) != len(away_indices_pair_opt):\n",
    "    print(\n",
    "        f\" DIFFERENCE DETECTED: Number of away index pairs doesn't match ({len(away_indices_pair_orig)} vs {len(away_indices_pair_opt)})\"\n",
    "    )\n",
    "else:\n",
    "    print(\" Number of away index pairs matches\")\n",
    "\n",
    "# Step 8: Extract sequences between away points\n",
    "print(\"\\nSTEP 8: Extract sequences between away points\")\n",
    "\n",
    "# Original version\n",
    "seq_bw_away = [\n",
    "    original_data.loc[x + 1 : y - 1, \"enc_dummy\"] for x, y in away_indices_pair_orig\n",
    "]\n",
    "print(f\"Original: Extracted {len(seq_bw_away)} sequences between 'away' points\")\n",
    "\n",
    "seq_bw_away_sub = [x for x in seq_bw_away if x.size > 1]\n",
    "print(f\"Original: After removing sequences with size  1: {len(seq_bw_away_sub)}\")\n",
    "\n",
    "enc_seq_orig = [x for x in seq_bw_away_sub if np.in1d(1, x)]\n",
    "print(f\"Original: After keeping only sequences with encounters: {len(enc_seq_orig)}\")\n",
    "\n",
    "# Print some details about these sequences\n",
    "if len(enc_seq_orig) > 0:\n",
    "    print(\"Original: First sequence details:\")\n",
    "    for i, seq in enumerate(enc_seq_orig[:1]):  # Just show the first one\n",
    "        print(f\"  Sequence {i + 1}: Length={len(seq)}, Values={seq.values}\")\n",
    "        print(f\"  Index values: {seq.index.values}\")\n",
    "        print(f\"  Contains {sum(seq == 1)} values of 1 (encounters)\")\n",
    "\n",
    "# Optimized version\n",
    "enc_seq_opt = []\n",
    "segment_details = []\n",
    "\n",
    "for i, (start_idx, end_idx) in enumerate(away_indices_pair_opt):\n",
    "    if end_idx - start_idx <= 1:  # Skip if too small\n",
    "        continue\n",
    "\n",
    "    segment = enc_dummy[start_idx:end_idx]\n",
    "    if 1.0 not in segment:  # Skip if no encounters\n",
    "        continue\n",
    "\n",
    "    enc_seq_opt.append(segment)\n",
    "    segment_details.append((start_idx, end_idx))\n",
    "\n",
    "print(f\"Optimized: Found {len(enc_seq_opt)} sequences with encounters\")\n",
    "\n",
    "if len(enc_seq_opt) > 0:\n",
    "    print(\"Optimized: First sequence details:\")\n",
    "    for i, seq in enumerate(enc_seq_opt[:1]):  # Just show the first one\n",
    "        start_idx, end_idx = segment_details[i]\n",
    "        print(\n",
    "            f\"  Sequence {i + 1}: Length={len(seq)}, Start={start_idx}, End={end_idx}\"\n",
    "        )\n",
    "        print(f\"  Values: {seq}\")\n",
    "        print(f\"  Contains {sum(seq == 1.0)} values of 1 (encounters)\")\n",
    "\n",
    "if len(enc_seq_orig) != len(enc_seq_opt):\n",
    "    print(\n",
    "        f\" DIFFERENCE DETECTED: Number of encounter sequences doesn't match ({len(enc_seq_orig)} vs {len(enc_seq_opt)})\"\n",
    "    )\n",
    "else:\n",
    "    print(\" Number of encounter sequences matches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 9: Calculate encounter start and end times\n",
      "Original: First encounter start time: 2022-05-02 16:17:17.602161\n",
      "Original: First encounter end time: 2022-05-02 16:17:31.802316\n",
      "Optimized: First encounter start time: 2022-05-02 16:17:17.602161\n",
      "Optimized: First encounter end time: 2022-05-02 16:17:31.802316\n",
      " First encounter start time matches\n",
      " First encounter end time matches\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Calculate encounter start times and end times\n",
    "print(\"\\nSTEP 9: Calculate encounter start and end times\")\n",
    "\n",
    "if len(enc_seq_orig) > 0:\n",
    "    # Original version\n",
    "    enc_start_times_orig = []\n",
    "    for seq in enc_seq_orig:\n",
    "        first_enc_idx = seq.index.values[np.where(seq == 1)[0][0]]\n",
    "        start_time = original_data.loc[first_enc_idx, \"Time\"]\n",
    "        enc_start_times_orig.append(start_time)\n",
    "\n",
    "    enc_end_times_orig = [\n",
    "        original_data.loc[seq.index.values[-1], \"Time\"] for seq in enc_seq_orig\n",
    "    ]\n",
    "\n",
    "    print(f\"Original: First encounter start time: {enc_start_times_orig[0]}\")\n",
    "    print(f\"Original: First encounter end time: {enc_end_times_orig[0]}\")\n",
    "\n",
    "    # Optimized version\n",
    "    enc_start_times_opt = []\n",
    "    enc_end_times_opt = []\n",
    "\n",
    "    for i, segment in enumerate(enc_seq_opt):\n",
    "        start_idx, end_idx = segment_details[i]\n",
    "        first_enc_offset = np.where(segment == 1.0)[0][0]\n",
    "        first_enc_idx = start_idx + first_enc_offset\n",
    "\n",
    "        start_time = optimized_data.loc[index_values[first_enc_idx], \"Time\"]\n",
    "        end_time = optimized_data.loc[index_values[end_idx - 1], \"Time\"]\n",
    "\n",
    "        enc_start_times_opt.append(start_time)\n",
    "        enc_end_times_opt.append(end_time)\n",
    "\n",
    "    print(f\"Optimized: First encounter start time: {enc_start_times_opt[0]}\")\n",
    "    print(f\"Optimized: First encounter end time: {enc_end_times_opt[0]}\")\n",
    "\n",
    "    if enc_start_times_orig[0] != enc_start_times_opt[0]:\n",
    "        print(\" DIFFERENCE DETECTED: First encounter start time doesn't match\")\n",
    "    else:\n",
    "        print(\" First encounter start time matches\")\n",
    "\n",
    "    if enc_end_times_orig[0] != enc_end_times_opt[0]:\n",
    "        print(\" DIFFERENCE DETECTED: First encounter end time doesn't match\")\n",
    "    else:\n",
    "        print(\" First encounter end time matches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 10: Calculate encounter durations\n",
      "Original: First encounter duration: 14.200155 seconds\n",
      "Optimized: First encounter duration: 14.200155 seconds\n",
      " First encounter duration matches\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Calculate encounter durations\n",
    "print(\"\\nSTEP 10: Calculate encounter durations\")\n",
    "\n",
    "if len(enc_seq_orig) > 0:\n",
    "    # Original version\n",
    "    enc_durations_orig = [\n",
    "        (end - start).total_seconds()\n",
    "        for start, end in zip(enc_start_times_orig, enc_end_times_orig)\n",
    "    ]\n",
    "    print(f\"Original: First encounter duration: {enc_durations_orig[0]} seconds\")\n",
    "\n",
    "    # Optimized version\n",
    "    enc_durations_opt = [\n",
    "        (end - start).total_seconds()\n",
    "        for start, end in zip(enc_start_times_opt, enc_end_times_opt)\n",
    "    ]\n",
    "    print(f\"Optimized: First encounter duration: {enc_durations_opt[0]} seconds\")\n",
    "\n",
    "    if (\n",
    "        abs(enc_durations_orig[0] - enc_durations_opt[0]) > 0.001\n",
    "    ):  # Allow small floating point differences\n",
    "        print(\n",
    "            f\" DIFFERENCE DETECTED: First encounter duration doesn't match ({enc_durations_orig[0]} vs {enc_durations_opt[0]})\"\n",
    "        )\n",
    "    else:\n",
    "        print(\" First encounter duration matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 11: Calculate encounter sub-sequences and durations (CRITICAL STEP)\n",
      "Original: First encounter has 2 sub-sequences\n",
      "Original: Total duration of sub-sequences: 1.600017 seconds\n",
      "Original: Sub-sequence start indices: [297324, 300462]\n",
      "Original: Sub-sequence end indices: [297461, 300854]\n",
      "Original: Individual sub-sequence durations: [0.400004, 1.200013]\n",
      "Optimized: Found 5 change points in first segment\n",
      "Optimized: Change indices: [ 0  1  4 39 46]\n",
      "Optimized: Values at change points:\n",
      "  Change 2: Index 1, Before=0.5, After=0.0\n",
      "  Change 3: Index 4, Before=1.0, After=1.0\n",
      "  Change 4: Index 39, Before=0.5, After=0.5\n",
      "  Change 5: Index 46, Before=1.0, After=1.0\n",
      "  Subsequence 1: Start idx=2012, value=0.0\n",
      "  Subsequence 2: Start idx=2016, value=1.0\n",
      "Optimized: First encounter has 2 sub-sequences\n",
      "Optimized: Total duration of sub-sequences: 8.600093 seconds\n",
      "Optimized: Sub-sequence start indices: [2012, 2016]\n",
      "Optimized: Sub-sequence end indices: [2012, 2050]\n",
      "Optimized: Individual sub-sequence durations: [0.0, 8.600093]\n",
      " Number of sub-sequences matches\n",
      " DIFFERENCE DETECTED: Total sub-sequence duration doesn't match (1.600017 vs 8.600093)\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Calculate encounter sub-sequences and their durations (the critical step)\n",
    "print(\"\\nSTEP 11: Calculate encounter sub-sequences and durations (CRITICAL STEP)\")\n",
    "\n",
    "if len(enc_seq_orig) > 0:\n",
    "    # Original version using time_within_encounter_threshold\n",
    "    def time_within_encounter_threshold(ds, encounter_sequence):\n",
    "        \"\"\"Original function to calculate sub-sequences when an individual is within the encounter threshold\"\"\"\n",
    "        # Add 0.5 to beginning and end for boundary detection\n",
    "        change_indices = np.where(\n",
    "            np.diff(np.concatenate(([0.5], encounter_sequence, [0.5]))) != 0\n",
    "        )[0]\n",
    "\n",
    "        # Get start indices (even-indexed change points)\n",
    "        start_indices = [\n",
    "            encounter_sequence.index.values[x] for x in change_indices[::2]\n",
    "        ]\n",
    "\n",
    "        # Get end indices (odd-indexed change points, minus 1 for inclusivity)\n",
    "        end_indices = [\n",
    "            encounter_sequence.index.values[x] for x in change_indices[1::2] - 1\n",
    "        ]\n",
    "\n",
    "        # Get times for start and end\n",
    "        start_times = [ds.loc[x, \"Time\"] for x in start_indices]\n",
    "        end_times = [ds.loc[x, \"Time\"] for x in end_indices]\n",
    "\n",
    "        # Calculate durations\n",
    "        enc_times = np.subtract(end_times, start_times)\n",
    "        enc_times_sec = [x.total_seconds() for x in enc_times]\n",
    "\n",
    "        # Total count and duration\n",
    "        total_enc = len(enc_times_sec)\n",
    "        total_enc_times = np.sum(enc_times_sec)\n",
    "\n",
    "        return total_enc, total_enc_times, start_indices, end_indices, enc_times_sec\n",
    "\n",
    "    # Apply to first sequence as example\n",
    "    first_seq = enc_seq_orig[0]\n",
    "    num_subseq_orig, total_duration_orig, starts_orig, ends_orig, durations_orig = (\n",
    "        time_within_encounter_threshold(original_data, first_seq)\n",
    "    )\n",
    "\n",
    "    print(f\"Original: First encounter has {num_subseq_orig} sub-sequences\")\n",
    "    print(f\"Original: Total duration of sub-sequences: {total_duration_orig} seconds\")\n",
    "    print(f\"Original: Sub-sequence start indices: {starts_orig}\")\n",
    "    print(f\"Original: Sub-sequence end indices: {ends_orig}\")\n",
    "    print(f\"Original: Individual sub-sequence durations: {durations_orig}\")\n",
    "\n",
    "    # Optimized version - let's extract just the first sequence to match\n",
    "    first_segment = enc_seq_opt[0]\n",
    "    start_idx, end_idx = segment_details[0]\n",
    "\n",
    "    # This is the critical part that differs in the optimized version:\n",
    "    # Find sub-sequences where ants are within encounter threshold (dummy=1)\n",
    "    bordered_segment = np.concatenate(([0.5], first_segment, [0.5]))\n",
    "    boundaries = np.diff(bordered_segment) != 0\n",
    "    change_indices = np.where(boundaries)[0]\n",
    "\n",
    "    print(f\"Optimized: Found {len(change_indices)} change points in first segment\")\n",
    "    print(f\"Optimized: Change indices: {change_indices}\")\n",
    "\n",
    "    # Check values at change points in bordered_segment\n",
    "    print(\"Optimized: Values at change points:\")\n",
    "    for i, idx in enumerate(change_indices):\n",
    "        if idx > 0 and idx < len(bordered_segment):\n",
    "            before = bordered_segment[idx - 1]\n",
    "            after = bordered_segment[idx]\n",
    "            print(f\"  Change {i + 1}: Index {idx}, Before={before}, After={after}\")\n",
    "\n",
    "    # Pair start and end indices of encounter sub-sequences\n",
    "    subsequence_count = len(change_indices) // 2\n",
    "    total_subsequence_duration = 0.0\n",
    "    subseq_start_indices = []\n",
    "    subseq_end_indices = []\n",
    "    subseq_durations = []\n",
    "\n",
    "    for j in range(subsequence_count):\n",
    "        subseq_start_idx = (\n",
    "            start_idx + change_indices[j * 2] - 1\n",
    "        )  # -1 to account for concatenation\n",
    "        subseq_end_idx = (\n",
    "            start_idx + change_indices[j * 2 + 1] - 1 - 1\n",
    "        )  # Additional -1 for inclusive end\n",
    "\n",
    "        # Check if we're only processing subsequences with value 1\n",
    "        value_at_start = (\n",
    "            enc_dummy[subseq_start_idx] if subseq_start_idx < len(enc_dummy) else None\n",
    "        )\n",
    "        print(\n",
    "            f\"  Subsequence {j + 1}: Start idx={subseq_start_idx}, value={value_at_start}\"\n",
    "        )\n",
    "\n",
    "        if subseq_end_idx >= subseq_start_idx:\n",
    "            subseq_start_indices.append(subseq_start_idx)\n",
    "            subseq_end_indices.append(subseq_end_idx)\n",
    "\n",
    "            subseq_start_time = optimized_data.loc[\n",
    "                index_values[subseq_start_idx], \"Time\"\n",
    "            ]\n",
    "            subseq_end_time = optimized_data.loc[index_values[subseq_end_idx], \"Time\"]\n",
    "            subseq_duration = (subseq_end_time - subseq_start_time).total_seconds()\n",
    "\n",
    "            subseq_durations.append(subseq_duration)\n",
    "            total_subsequence_duration += subseq_duration\n",
    "\n",
    "    print(f\"Optimized: First encounter has {subsequence_count} sub-sequences\")\n",
    "    print(\n",
    "        f\"Optimized: Total duration of sub-sequences: {total_subsequence_duration} seconds\"\n",
    "    )\n",
    "    print(f\"Optimized: Sub-sequence start indices: {subseq_start_indices}\")\n",
    "    print(f\"Optimized: Sub-sequence end indices: {subseq_end_indices}\")\n",
    "    print(f\"Optimized: Individual sub-sequence durations: {subseq_durations}\")\n",
    "\n",
    "    if num_subseq_orig != subsequence_count:\n",
    "        print(\n",
    "            f\" DIFFERENCE DETECTED: Number of sub-sequences doesn't match ({num_subseq_orig} vs {subsequence_count})\"\n",
    "        )\n",
    "    else:\n",
    "        print(\" Number of sub-sequences matches\")\n",
    "\n",
    "    if abs(total_duration_orig - total_subsequence_duration) > 0.001:\n",
    "        print(\n",
    "            f\" DIFFERENCE DETECTED: Total sub-sequence duration doesn't match ({total_duration_orig} vs {total_subsequence_duration})\"\n",
    "        )\n",
    "    else:\n",
    "        print(\" Total sub-sequence duration matches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEEP INVESTIGATION OF SUBSEQUENCE IDENTIFICATION:\n",
      "Original: Detailed analysis of first encounter sequence\n",
      "  Sequence values: [1.  1.  1.  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 1.  1.  1.  1.  1.  1.  1.  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "  Sequence indices: [297324 297394 297461 297600 297738 297808 297877 297946 298012 298075\n",
      " 298143 298212 298283 298351 298418 298484 298550 298616 298681 298749\n",
      " 298817 298888 298958 299025 299091 299160 299232 299299 299363 299428\n",
      " 299491 299556 299623 300075 300138 300267 300330 300398 300462 300525\n",
      " 300588 300654 300721 300787 300854 300919 300985 301052 301119 301185\n",
      " 301252 301316 301382 301448 301512 301577 301640 301704 301771 301839\n",
      " 301908 301977 302044]\n"
     ]
    }
   ],
   "source": [
    "# Deep investigation of the subsequence identification\n",
    "print(\"\\nDEEP INVESTIGATION OF SUBSEQUENCE IDENTIFICATION:\")\n",
    "print(\"Original: Detailed analysis of first encounter sequence\")\n",
    "enc_dummy_values = first_seq.values\n",
    "print(f\"  Sequence values: {enc_dummy_values}\")\n",
    "print(f\"  Sequence indices: {first_seq.index.values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEEP INVESTIGATION OF SUBSEQUENCE IDENTIFICATION:\n",
      "Original: Detailed analysis of first encounter sequence\n",
      "  Sequence values: [1.  1.  1.  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 1.  1.  1.  1.  1.  1.  1.  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "  Sequence indices: [297324 297394 297461 297600 297738 297808 297877 297946 298012 298075\n",
      " 298143 298212 298283 298351 298418 298484 298550 298616 298681 298749\n",
      " 298817 298888 298958 299025 299091 299160 299232 299299 299363 299428\n",
      " 299491 299556 299623 300075 300138 300267 300330 300398 300462 300525\n",
      " 300588 300654 300721 300787 300854 300919 300985 301052 301119 301185\n",
      " 301252 301316 301382 301448 301512 301577 301640 301704 301771 301839\n",
      " 301908 301977 302044]\n",
      "\n",
      "Original sequence visualization (1=encounter, 0.5=between thresholds):\n",
      "EEEBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBEEEEEEEBBBBBBBBBBBBBBBBBB\n",
      "\n",
      "Optimized sequence visualization:\n",
      "AEEEBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBEEEEEEEBBBBBBBBBBBBBBBBBB\n",
      "Original: Changes after concatenation at indices: [ 0  3 38 45]\n",
      "Optimized: Changes after concatenation at indices: [ 0  1  4 39 46]\n",
      "\n",
      "COMPARISON OF CRITICAL SUBSEQUENCE CALCULATIONS:\n",
      "Original:\n",
      "  Pair 1: Start=0, End=3\n",
      "  Values: Start=0.5, End=1.0\n",
      "  Pair 2: Start=38, End=45\n",
      "  Values: Start=0.5, End=1.0\n",
      "Optimized:\n",
      "  Pair 1: Start=0, End=1\n",
      "  Values: Start=0.5, End=0.0\n",
      "  Pair 2: Start=4, End=39\n",
      "  Values: Start=1.0, End=0.5\n",
      "\n",
      "FINAL OUTPUT FOR FIRST ENCOUNTER:\n",
      "Original: 2 subsequences with total duration 1.600017 seconds\n",
      "Optimized: 2 subsequences with total duration 8.600093 seconds\n"
     ]
    }
   ],
   "source": [
    "# Deep investigation of the subsequence identification\n",
    "print(\"\\nDEEP INVESTIGATION OF SUBSEQUENCE IDENTIFICATION:\")\n",
    "print(\"Original: Detailed analysis of first encounter sequence\")\n",
    "enc_dummy_values = first_seq.values\n",
    "print(f\"  Sequence values: {enc_dummy_values}\")\n",
    "print(f\"  Sequence indices: {first_seq.index.values}\")\n",
    "\n",
    "# Create a diagram to visualize the sequence\n",
    "print(\"\\nOriginal sequence visualization (1=encounter, 0.5=between thresholds):\")\n",
    "enc_str = \"\"\n",
    "for val in enc_dummy_values:\n",
    "    if val == 1.0:\n",
    "        enc_str += \"E\"  # Encounter\n",
    "    elif val == 0.5:\n",
    "        enc_str += \"B\"  # Between thresholds\n",
    "    else:\n",
    "        enc_str += \"A\"  # Away\n",
    "print(enc_str)\n",
    "\n",
    "print(\"\\nOptimized sequence visualization:\")\n",
    "opt_str = \"\"\n",
    "for val in first_segment:\n",
    "    if val == 1.0:\n",
    "        opt_str += \"E\"  # Encounter\n",
    "    elif val == 0.5:\n",
    "        opt_str += \"B\"  # Between thresholds\n",
    "    else:\n",
    "        opt_str += \"A\"  # Away\n",
    "print(opt_str)\n",
    "\n",
    "# Check the behavior with the concatenation and diff\n",
    "orig_concat = np.concatenate(([0.5], enc_dummy_values, [0.5]))\n",
    "orig_diff = np.diff(orig_concat)\n",
    "orig_changes = np.where(orig_diff != 0)[0]\n",
    "print(f\"Original: Changes after concatenation at indices: {orig_changes}\")\n",
    "\n",
    "opt_concat = np.concatenate(([0.5], first_segment, [0.5]))\n",
    "opt_diff = np.diff(opt_concat)\n",
    "opt_changes = np.where(opt_diff != 0)[0]\n",
    "print(f\"Optimized: Changes after concatenation at indices: {opt_changes}\")\n",
    "\n",
    "# Compare critical calculations\n",
    "print(\"\\nCOMPARISON OF CRITICAL SUBSEQUENCE CALCULATIONS:\")\n",
    "print(\"Original:\")\n",
    "for i in range(0, len(orig_changes), 2):\n",
    "    if i + 1 < len(orig_changes):\n",
    "        print(\n",
    "            f\"  Pair {i // 2 + 1}: Start={orig_changes[i]}, End={orig_changes[i + 1]}\"\n",
    "        )\n",
    "        # What values are at these points?\n",
    "        start_val = (\n",
    "            orig_concat[orig_changes[i]]\n",
    "            if orig_changes[i] < len(orig_concat)\n",
    "            else \"out of bounds\"\n",
    "        )\n",
    "        end_val = (\n",
    "            orig_concat[orig_changes[i + 1]]\n",
    "            if orig_changes[i + 1] < len(orig_concat)\n",
    "            else \"out of bounds\"\n",
    "        )\n",
    "        print(f\"  Values: Start={start_val}, End={end_val}\")\n",
    "\n",
    "print(\"Optimized:\")\n",
    "for i in range(0, len(opt_changes), 2):\n",
    "    if i + 1 < len(opt_changes):\n",
    "        print(f\"  Pair {i // 2 + 1}: Start={opt_changes[i]}, End={opt_changes[i + 1]}\")\n",
    "        # What values are at these points?\n",
    "        start_val = (\n",
    "            opt_concat[opt_changes[i]]\n",
    "            if opt_changes[i] < len(opt_concat)\n",
    "            else \"out of bounds\"\n",
    "        )\n",
    "        end_val = (\n",
    "            opt_concat[opt_changes[i + 1]]\n",
    "            if opt_changes[i + 1] < len(opt_concat)\n",
    "            else \"out of bounds\"\n",
    "        )\n",
    "        print(f\"  Values: Start={start_val}, End={end_val}\")\n",
    "\n",
    "# Final output for the first encounter\n",
    "print(\"\\nFINAL OUTPUT FOR FIRST ENCOUNTER:\")\n",
    "print(\n",
    "    f\"Original: {num_subseq_orig} subsequences with total duration {total_duration_orig} seconds\"\n",
    ")\n",
    "print(\n",
    "    f\"Optimized: {subsequence_count} subsequences with total duration {total_subsequence_duration} seconds\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
